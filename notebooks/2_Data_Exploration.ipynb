{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4981038e-6c84-4135-8bd7-e9951ee239d6",
   "metadata": {},
   "source": [
    "# 🏞️ Part 2: Data Exploration\n",
    "Before we begin, we are going to load some of the key libraries into our environment. There is no need to memorise them as we go along, but here is a brief list:\n",
    "\n",
    "| Library | Description |\n",
    "| --- | --- |\n",
    "| `torch` | This is the core `PyTorch` library doing most of the heavy lifting |\n",
    "| `torchvision` | This is how we will extract the data (if needed), and apply some cleaning to images |\n",
    "| `random` | This library provides random number generators. This is important if we want to sample images from our dataset | \n",
    "| `matplotlib.pyplot` | This is a very popular plotting library |\n",
    "| `time` | This has functions which allows us to keep track of how long the code takes to run |\n",
    "| `IPython` | This libraries allows us to access some of the options of these notebooks |\n",
    "| `numpy` | This is a very popular data manipulation library |\n",
    "| `helper` | A user-defined module (see the `./helper` subfolder for more details) of useful functions and classes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b33c148-5646-45a1-91e0-73bde9be79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from helper import helper\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2a17-3531-463b-89d9-b6cf3a035790",
   "metadata": {},
   "source": [
    "## ⚗️ The Data Science Pipleine \n",
    "*This section will be repeated in both Part 2 and Part 3*\n",
    "\n",
    "> What on earth is data science?! -- George Washington (probably not)\n",
    "\n",
    "Seriously though, nowadays, in such a data-rich world, data science has become the new buzzword, the new cool kid in the block. But what exactly is it? Unfortunately, no one can really pin down a [rigourous definition](https://hdsr.mitpress.mit.edu/pub/jhy4g6eg/release/7) of data science. At the high level:\n",
    "\n",
    "> Data science is the systematic extraction of novel insight from data.\n",
    "\n",
    "Good enough! With this definition, most practitioners can somewhat agree on a pipeline or flow. Here are the steps:\n",
    "1. Identify your problem (What are you trying to do?)\n",
    "2. Obtain your data (What resource do we have to work with?)\n",
    "3. Explore your data (What does our data actually look like?)\n",
    "4. Prepare your data (How do we clean/wrangle our data to make it ingestible?)\n",
    "5. Model your data (How do we automate the process of drawing out insights?)\n",
    "6. Evaluate your model (How good are our predictions?)\n",
    "7. Deploy your model (How can the wider-user base access these insights?)\n",
    "\n",
    "The 7th step is out-of-scope for this workshop, but we well be exploring the other steps to varying degrees:\n",
    "* Steps 1-4 will be explored in Part 2.\n",
    "* Steps 5-6 will be explored in Part 3 and 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5ba8e-6ae6-47b3-bbf5-8fb23a060fae",
   "metadata": {},
   "source": [
    "## Step 1: Identify Your Problem \n",
    "![cc](../images/confused_cat.jpg)\n",
    "\n",
    "**Figure:** A day-to-day snapshot of a data scientist at work. ([source](https://s.keepmeme.com/files/en_posts/20200925/confused-cat-looking-at-computer-with-a-lot-of-question-marks-meme-861f3efff59aedea603e35b8c3c059f0.jpg))\n",
    "\n",
    "The problem:\n",
    "* Your boss comes up to you and gives you a stack of black & white photos\n",
    "* You get told you need to identify what item of clothing each photo represents (eg. t-shirt)\n",
    "* You get a stack of 70,000 labelled picture to give you an idea of the task\n",
    "\n",
    "*What do you do?!* You can label them by hand if there are only 100 or 1000 unlabelled images. But what if there are 1,000,000? This manual labelling is not tenable in the long term.\n",
    "\n",
    "Why use automated image recognition?\n",
    "* **Scalable** -- provided you have a reasonable model and enough computational resources, getting a computer to label images is ***a lot*** easier.\n",
    "* **Consistent** -- the output of the model is going to be more consistent than any crack team of labelers you can assemble (we are but humans after all)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dac7d9-0306-4f4a-840b-70872631c283",
   "metadata": {},
   "source": [
    "## Step 2: Obtain Your Data\n",
    "Alright, you decided you probably won't manually label \\* phew \\*. What next? Lucky for you, your boss provided you with some initial information:\n",
    "* There are 70,000 labelled images.\n",
    "* Each image is an item of clothing.\n",
    "* Each image is a 28x28 sized image (784 pixels in total).\n",
    "* Each pixel is black and white, and has a value between 0 and 255 indicating the brightness of the pixel\n",
    "* There are a total of 10 types of items of clothing.\n",
    "\n",
    "| Label | Type of Clothing |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat | \n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle Boot |\n",
    "\n",
    "For example, the following is an image of a boot:\n",
    "\n",
    "![Boot](../images/boot.png)\n",
    "\n",
    "In fact, the dataset we are working with is called the [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist). We will write a function below to extract all the labelled data into either train or test sets (`train_iter` and `test_iter` respectively). Don't worry too much about the details of the function, the key point is the input parameters. In Step 4, we will discuss why a split between train and test is necessary in the first place.\n",
    "\n",
    "The function `load_data_fashion_mnist` takes in two input arguments. But don't worry too much about what they mean, we can use some set values.\n",
    "* `batch_size`: The number of examples we use in each step of the gradient descent. There is a general trade off:\n",
    "    * Small batch size: Less stable training (since our gradient descent is reliant on very few examples)\n",
    "    * Large batch size: Tends to be a bit slower and requires a bit more memory.\n",
    "* and `n_workers`: The number of computer (CPU or GPU) cores engaged in reading the data.\n",
    "\n",
    "The recommended argument will be different depending on whether you are running this on mybinder or your own machine.\n",
    "\n",
    "#### For mybinder users\n",
    "Due to limited resources, these values seem to provide a stable outcome:\n",
    "* `batch_size = 1024`\n",
    "* `n_workers = 0`\n",
    "\n",
    "#### For those on their own machines\n",
    "You should have more resources than mybinder, we recommend the following:\n",
    "* `batch_size = 256`\n",
    "* `n_workers = 4`\n",
    "\n",
    "Note if you are using GPU acceleration, you may choose to use more worker nodes. See the Appendix for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8d2fcc-7942-4df7-9461-dcedec864de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the function without running it\n",
    "def load_data_fashion_mnist(batch_size, n_workers):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))\n",
    "\n",
    "# Then execute the function here\n",
    "batch_size = 1024  # Set to 256 on your own device\n",
    "n_workers = 0      # Set to 4 on your own device\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1919da-70f7-4174-b168-6aac3bea2c02",
   "metadata": {},
   "source": [
    "Note that for this particular example, we didn't have to explicitly split the data into train and test set, it came pre-split:\n",
    "* 60,000 images into `train_iter`\n",
    "* 10,000 images into `test_iter`\n",
    "\n",
    "Let's see if we can have a look at it with a `print()` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed9d889-2cb4-43e5-bdec-1bc13489829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AF0A8F69D0>\n"
     ]
    }
   ],
   "source": [
    "print(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112a5c3-4ccb-446e-98c5-b95d3d6738b0",
   "metadata": {},
   "source": [
    "Weird, it's not printing out the mini-batch, but rather some weird thing called a `DataLoader object`. There's actually a really good reason for this! If we are dealing with big datasets, reading the data in as a table would mean we need to store the entire thing in memory (RAM) - this can be up to 10-20GB which can cripple a normal computer! Instead we use the `DataLoader object` to read in the dataset *as we need it* (and discarding it otherwise). This might be a bit slower, but far more manageable in terms of memory resources. This means there's a very particular way to extract a single example, which we will attempt to do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e3e7bc-cf6e-4a93-8a14-dd269bb1187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batches we have extracted are: 3\n",
      "Out of the 3 mini-batches, we have selected the 3 th one.\n",
      "The number of images in the mini-batch we selected are: 1024  and  1024 . Note these two values should be equal.\n",
      "The shape of the predictor torch.Size([1, 28, 28])\n",
      "The shape of the label torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# First we extract a few random batches (say 3), each of which will have up to your set batch_size (eg. 1024 for mybinder)\n",
    "sampled_batches = random.sample(list(train_iter), 3)\n",
    "print(\"The number of mini-batches we have extracted are:\", len(sampled_batches))\n",
    "\n",
    "# Second we select a single batch to look at, let's say the 3rd one\n",
    "batch_no = 2\n",
    "## 0 denotes the predictors\n",
    "## 1 denotes the labels\n",
    "predictor = sampled_batches[batch_no][0]\n",
    "label = sampled_batches[batch_no][1]\n",
    "print(\"Out of the\", len(sampled_batches), \"mini-batches, we have selected the\", batch_no + 1, \"th one.\")\n",
    "print(\"The number of images in the mini-batch we selected are:\" , len(predictor), \" and \", len(label), \". Note these two values should be equal.\")\n",
    "\n",
    "# Third, we select a single example in the batch, let's say the 100th one\n",
    "example_no = 99\n",
    "single_predictor = predictor[example_no]\n",
    "single_label = label[example_no]\n",
    "print(\"The shape of the predictor\", single_predictor.shape)\n",
    "print(\"The shape of the label\", single_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a4ff0-df38-4d26-8589-36e90b8e051d",
   "metadata": {},
   "source": [
    "In the last two lines, we have printed the `shape` of our selected example/image:\n",
    "* The predictor has `[1, 28, 28]`:\n",
    "    * The `1` represents the number of channels (since it's black and white this is only 1. For colour, it's 3 RGB).\n",
    "    * The `28` and `28` represents the dimension of the image (28x28)\n",
    "* The label has `[]`: this means it's just a number\n",
    "\n",
    "Let's try to show the predictor and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efbdce18-f4a0-49d6-9700-edb2efd592ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows an image of:  8\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(single_predictor[0]) # print only the one channel of BW\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows an image of: \", int(single_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acb39c-fe12-4b4b-96ee-00eeacb7edd6",
   "metadata": {},
   "source": [
    "Check if this makes sense according to the label table above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b468ca6-1978-4525-8b2d-3a401deb2212",
   "metadata": {},
   "source": [
    "## Step 3: Explore Your Data\n",
    "The Fashion MNIST data we have is relatively clean. Not much needs to be done here. But let's have a look at the class distribution of the train/test data. It is important that the training data has enough examples to 'learn' the patterns of each class. Ideally a balanced class (with similar number of examples for each) is best.\n",
    "\n",
    "Let's see what the class distribution is like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1885bcec-2eaf-4e4d-9dfa-6132033f635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Class 0 has 6000 images\n",
      "Class 1 has 6000 images\n",
      "Class 2 has 6000 images\n",
      "Class 3 has 6000 images\n",
      "Class 4 has 6000 images\n",
      "Class 5 has 6000 images\n",
      "Class 6 has 6000 images\n",
      "Class 7 has 6000 images\n",
      "Class 8 has 6000 images\n",
      "Class 9 has 6000 images\n",
      "Test Data\n",
      "Class 0 has 1000 images\n",
      "Class 1 has 1000 images\n",
      "Class 2 has 1000 images\n",
      "Class 3 has 1000 images\n",
      "Class 4 has 1000 images\n",
      "Class 5 has 1000 images\n",
      "Class 6 has 1000 images\n",
      "Class 7 has 1000 images\n",
      "Class 8 has 1000 images\n",
      "Class 9 has 1000 images\n"
     ]
    }
   ],
   "source": [
    "def data_explore(data_iter):\n",
    "    class_count = helper.Accumulator(10)\n",
    "    for i, (X, y) in enumerate(data_iter):\n",
    "        current_counter = torch.bincount(y)\n",
    "        class_count.add(current_counter[0], current_counter[1], current_counter[2], current_counter[3], current_counter[4],\n",
    "                  current_counter[5], current_counter[6], current_counter[7], current_counter[8], current_counter[9]) # This is bad coding practice, don't do this!\n",
    "    for i in range(10):\n",
    "        print(\"Class\", i, \"has\", int(class_count.__getitem__(i)), \"images\")\n",
    "        \n",
    "    return\n",
    "        \n",
    "print(\"Train Data:\")\n",
    "data_explore(train_iter)\n",
    "print(\"Test Data\")\n",
    "data_explore(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d3a7c-b818-48ac-ab9d-c352340d9d38",
   "metadata": {},
   "source": [
    "![](../images/thanos.jpg)\n",
    "\n",
    "([source](https://i.kym-cdn.com/entries/icons/original/000/027/257/perfectly-balanced-as-all-things-should-be.jpg))\n",
    "\n",
    "In reality, your data won't be this perfect, it's always worth checking and understanding class balances!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821303b4-092e-45cd-9b32-6448122e0166",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Your Data\n",
    "This was something we have previously alluded to, but we have 70,000 labelled examples. Perfect! Do we throw them all into the training/fitting process of the model?\n",
    "\n",
    "The answer is **NO**. Why is that though? Doesn't more data = better model?\n",
    "\n",
    "![yesbutno](../images/yesbutno.jpg)\n",
    "\n",
    "The reason lies in a concept known as **overfitting**. Remember, our goal is to make sure the model works well on hiterto unseen data (ie. unlabelled data). If we just throw all the data we have into the train process, then we won't have access to an independent dataset to evaluate the model. Later on, we will see how the test data can be used to assess overfitting. For now, just remember that it is vital that we have an (representative) subset of the labelled data set aside for evaluation. For the purposes of this example, 60,000 images will be used to train and 10,000 images will be used to test:\n",
    "* `Train` -- this is the data we use to fit the model (n=60,000)\n",
    "* `Validate` -- we will not be using this today to tune hyper-parameters\n",
    "* `Test` -- this is the data we use to determine the model performance (n=10,000)\n",
    "\n",
    "#### Aside: Validate \\& Hyperparameters\n",
    "**Hyperparameters** are parameters of the model that the user has to provide (as opposed to parameters the model finds such as the weights of a neural network). Examples of this might be the learning rate of the gradient descent method (denoted by `lr` later). Since these parameters have to be provided by the user, we need to decide on its value somehow. In the absence of any external information (eg. someone told us a particular learning rate is really good), we use the validate set (independent of train and test) to set these. In the example of learning rate, we may try the learning rate in increments of `0.1` between `0.1` and `0.9`. Then we choose the hyperparameter which gives us the best performance when assessed against the validate set.\n",
    "\n",
    "*For the purpose of this tutorial, we will not do any hyperparameter tuning, so a validate set was not used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3acd82-c9df-42b1-93b5-5cb6b97155d3",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
