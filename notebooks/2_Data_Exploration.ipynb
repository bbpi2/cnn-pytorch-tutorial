{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4981038e-6c84-4135-8bd7-e9951ee239d6",
   "metadata": {},
   "source": [
    "# üèûÔ∏è Part 2: Data Exploration\n",
    "With the basics out of the way, we can now jump into image recognition. Before we begin, we are going to load some useful Python libraries into our environment. You don't need to have a precise understanding of what each library does, but here is a rough summary.\n",
    "\n",
    "| Library | Description |\n",
    "| --- | --- |\n",
    "| `torch` | This is the core `PyTorch` library doing most of the heavy lifting |\n",
    "| `torchvision` | This is how we will extract the data (if needed), and apply some cleaning to images |\n",
    "| `random` | This library provides random number generators. This is important if we want to sample images from our dataset | \n",
    "| `matplotlib.pyplot` | This is a very popular plotting library |\n",
    "| `time` | This has functions which allows us to keep track of how long the code takes to run |\n",
    "| `IPython` | This libraries allows us to access some of the options of these notebooks |\n",
    "| `numpy` | This is a very popular data manipulation library |\n",
    "| `helper` | A user-defined module (see the `./helper` subfolder for more details) of useful functions and classes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b33c148-5646-45a1-91e0-73bde9be79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from helper import helper\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2a17-3531-463b-89d9-b6cf3a035790",
   "metadata": {},
   "source": [
    "## ‚öóÔ∏è The Data Science Pipleine \n",
    "*This section will be repeated in both Part 2 and Part 3*\n",
    "\n",
    "> What on earth is data science?! -- George Washington (probably not)\n",
    "\n",
    "Seriously though, nowadays, in such a data-rich world, data science has become the new buzzword, the new cool kid in the block. But what exactly is it? Unfortunately, no one can really pin down a [rigourous definition](https://hdsr.mitpress.mit.edu/pub/jhy4g6eg/release/7) of data science. At the high level:\n",
    "\n",
    "> Data science is the systematic extraction of novel insight from data.\n",
    "\n",
    "Good enough! With this definition, most practitioners can somewhat agree on a pipeline or flow. Here are the steps:\n",
    "1. Identify your problem (What are you trying to do?)\n",
    "2. Obtain your data (What resource do we have to work with?)\n",
    "3. Explore your data (What does our data actually look like?)\n",
    "4. Prepare your data (How do we clean/wrangle our data to make it ingestible?)\n",
    "5. Model your data (How do we automate the process of drawing out insights?)\n",
    "6. Evaluate your model (How good are our predictions?)\n",
    "7. Deploy your model (How can the wider-user base access these insights?)\n",
    "\n",
    "The 7th step is out-of-scope for this workshop, but we well be exploring the other steps to varying degrees:\n",
    "* Steps 1-4 will be explored in Part 2.\n",
    "* Steps 5-6 will be explored in Part 3 and Part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5ba8e-6ae6-47b3-bbf5-8fb23a060fae",
   "metadata": {},
   "source": [
    "## Step 1: Identify Your Problem \n",
    "![cc](../images/confused_cat.jpg)\n",
    "\n",
    "**Figure:** A day-to-day snapshot of a data scientist at work. ([source](https://s.keepmeme.com/files/en_posts/20200925/confused-cat-looking-at-computer-with-a-lot-of-question-marks-meme-861f3efff59aedea603e35b8c3c059f0.jpg))\n",
    "\n",
    "### The problem:\n",
    "* Your boss comes up to you and gives you a stack of unlabelled black & white photos\n",
    "* You get told you need to identify what item of clothing each photo represents (eg. t-shirt)\n",
    "* You get a stack of 70,000 labelled picture to give you an idea of the task\n",
    "\n",
    "*What do you do?!* Sure, you can label them by hand if there are only 100 or 1000 unlabelled images. But what if there are 1,000,000? This manual labelling is not tenable in the long term.\n",
    "\n",
    "Why use machine learning to automated image recognition?\n",
    "* **Scalable** -- provided you have a reasonable model and enough computational resources, getting a computer to label images is ***a lot*** easier.\n",
    "* **Consistent** -- the output of the model is going to be more consistent than any crack team of labelers you can assemble (we are but humans after all)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dac7d9-0306-4f4a-840b-70872631c283",
   "metadata": {},
   "source": [
    "## Step 2: Obtain Your Data\n",
    "Alright, you decided you probably won't manually label \\* phew \\*. What next? Lucky for you, your boss provided you with some initial information:\n",
    "* There are 70,000 labelled images.\n",
    "* Each image is an item of clothing.\n",
    "* Each image is a 28x28 sized image (784 pixels in total).\n",
    "* Each pixel is black and white, and has a value between 0 and 255 indicating the brightness of the pixel\n",
    "* There are a total of 10 types of items of clothing.\n",
    "\n",
    "| Label | Type of Clothing |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat | \n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle Boot |\n",
    "\n",
    "For example, the following is an image of a boot:\n",
    "\n",
    "![Boot](../images/boot.png)\n",
    "\n",
    "In fact, the dataset we are working with is called the [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist). In the cell below, we will try to extract the data and split them into the train and test sets (`train_iter` and `test_iter` respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39b273-db1c-4770-845e-174f641c2f0d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "\n",
    "Why do we need to split the labelled data into train/test sets? (Don't worry, we will go through this in Step 4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f64ea0-40e3-4bcc-817f-d682188761c5",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è If you clicked ![Binder](https://mybinder.org/badge_logo.svg) to get into the notebook, safely ignore the following. If you have opted to use your own anaconda to run this, we recommend the following parameters:\n",
    "* `batch_size = 256`\n",
    "* `n_workers = 4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8d2fcc-7942-4df7-9461-dcedec864de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the function without running it\n",
    "def load_data_fashion_mnist(batch_size, n_workers):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))\n",
    "\n",
    "# Then execute the function here\n",
    "batch_size = 1024  # Set to 256 on your own device\n",
    "n_workers = 0      # Set to 4 on your own device\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacd7c5-f225-47bf-a2ef-800b8467585c",
   "metadata": {},
   "source": [
    "#### üéâüéâüéâ Congratulations! You have just read in your train and test data! üéâüéâüéâ\n",
    "The format of `train_iter` and `test_iter` is a bit strange, so we have written a function to extract a single example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75593710-a031-4557-b938-a7a444c53c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_sample(data):\n",
    "    \"\"\" Extract a single example from train_iter or test_iter\"\"\"\n",
    "    # First we extract a few random batches (say 3), each of which will have up to your set batch_size (eg. 1024 for mybinder)\n",
    "    sampled_batches = random.sample(list(data), 3)\n",
    "    print(\"The number of mini-batches we have extracted are:\", len(sampled_batches))\n",
    "\n",
    "    # Second we select a single batch to look at, let's say the 3rd one\n",
    "    batch_no = 2\n",
    "    ## 0 denotes the predictors\n",
    "    ## 1 denotes the labels\n",
    "    predictor = sampled_batches[batch_no][0]\n",
    "    label = sampled_batches[batch_no][1]\n",
    "    print(\"Out of the\", len(sampled_batches), \"mini-batches, we have selected the\", batch_no + 1, \"th one.\")\n",
    "    print(\"The number of images in the mini-batch we selected are:\" , len(predictor), \" and \", len(label), \". Note these two values should be equal.\")\n",
    "\n",
    "    # Third, we select a single example in the batch, let's say the 100th one\n",
    "    example_no = 99\n",
    "    single_predictor = predictor[example_no]\n",
    "    single_label = label[example_no]\n",
    "    return (single_predictor, single_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e3e7bc-cf6e-4a93-8a14-dd269bb1187e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batches we have extracted are: 3\n",
      "Out of the 3 mini-batches, we have selected the 3 th one.\n",
      "The number of images in the mini-batch we selected are: 1024  and  1024 . Note these two values should be equal.\n",
      "The shape of the predictor torch.Size([1, 28, 28])\n",
      "The shape of the label torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "single_predictor, single_label = get_single_sample(train_iter)\n",
    "\n",
    "print(\"The shape of the predictor\", single_predictor.shape)\n",
    "print(\"The shape of the label\", single_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ce384-34a9-491f-8ff4-8b8a4eb7a0f5",
   "metadata": {},
   "source": [
    "The `shape()` function shows us the **dimensions** of our arrays:\n",
    "* The predictor has `[1, 28, 28]`: this is a tensor\n",
    "    * The `1` represents the number of channels (since it's black and white this is only =1. For colour, it =3 since it's RGB).\n",
    "    * The `28` and `28` represents the dimension of the image (28x28)\n",
    "* The label has `[]`: this is a scalar\n",
    "\n",
    "Now let's try to visual it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbdce18-f4a0-49d6-9700-edb2efd592ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows an image of:  1\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(single_predictor[0]) # print only the one channel of BW\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows an image of: \", int(single_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acb39c-fe12-4b4b-96ee-00eeacb7edd6",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Does the image match the label? (See table above).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b468ca6-1978-4525-8b2d-3a401deb2212",
   "metadata": {},
   "source": [
    "## Step 3: Explore Your Data\n",
    "Before jumping the gun to model our data, we need to process the data so that it's ready to be modelled. This process is called **data cleaning** and this can be a very involved process, beyond the scope of the workshop. For those interested, [this textbook](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) provides great worked examples of cleaning. Step 4 will touch on some of the additional cleaning/processing steps.\n",
    "\n",
    "Before even cleaning the data though, we need to ensure we understand what is going in our data (often times we don't collect it ourselves). Here we will focus on looking at the **class distribution**. This is effectively asking ourselves how many of each category is represented in our data. For example, are there more boots than there are t-shirts?\n",
    "\n",
    "For our model to work, we need the class distribution in the train set to be **similar** to that of the test set. Theoretically, if we just randomly split our labelled data, this will do just fine, but it's always worthwhile to check. Note that you don't need an *exact* match (in reality that will never happen!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f070b3f-a24b-4d1d-a20d-5dc8c9951fcf",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why do we prefer the train and test set to have similar class distributions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1885bcec-2eaf-4e4d-9dfa-6132033f635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Class 0 has 6000 images\n",
      "Class 1 has 6000 images\n",
      "Class 2 has 6000 images\n",
      "Class 3 has 6000 images\n",
      "Class 4 has 6000 images\n",
      "Class 5 has 6000 images\n",
      "Class 6 has 6000 images\n",
      "Class 7 has 6000 images\n",
      "Class 8 has 6000 images\n",
      "Class 9 has 6000 images\n",
      "Test Data\n",
      "Class 0 has 1000 images\n",
      "Class 1 has 1000 images\n",
      "Class 2 has 1000 images\n",
      "Class 3 has 1000 images\n",
      "Class 4 has 1000 images\n",
      "Class 5 has 1000 images\n",
      "Class 6 has 1000 images\n",
      "Class 7 has 1000 images\n",
      "Class 8 has 1000 images\n",
      "Class 9 has 1000 images\n"
     ]
    }
   ],
   "source": [
    "def data_explore(data_iter):\n",
    "    class_count = helper.Accumulator(10)\n",
    "    for i, (X, y) in enumerate(data_iter):\n",
    "        current_counter = torch.bincount(y)\n",
    "        class_count.add(current_counter[0], current_counter[1], current_counter[2], current_counter[3], current_counter[4],\n",
    "                  current_counter[5], current_counter[6], current_counter[7], current_counter[8], current_counter[9]) # This is bad coding practice, don't do this!\n",
    "    for i in range(10):\n",
    "        print(\"Class\", i, \"has\", int(class_count.__getitem__(i)), \"images\")\n",
    "        \n",
    "    return\n",
    "        \n",
    "print(\"Train Data:\")\n",
    "data_explore(train_iter)\n",
    "print(\"Test Data\")\n",
    "data_explore(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d3a7c-b818-48ac-ab9d-c352340d9d38",
   "metadata": {},
   "source": [
    "![](../images/thanos.jpg)  \n",
    "([source](https://i.kym-cdn.com/entries/icons/original/000/027/257/perfectly-balanced-as-all-things-should-be.jpg))\n",
    "\n",
    "In reality, your data won't be this perfect, it's always worth checking and understanding class balances!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821303b4-092e-45cd-9b32-6448122e0166",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Your Data\n",
    "This was something we have previously alluded to, but we have 70,000 labelled examples. Perfect! Do we throw them all into the training/fitting process of the model?\n",
    "\n",
    "The answer is **NO**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c052289-dcb5-432d-9053-9646d9e1456f",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "Why is that though? Doesn't more data = better model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba420027-5351-4c8b-a3fa-18623ce53076",
   "metadata": {},
   "source": [
    "![yesbutno](../images/yesbutno.jpg)\n",
    "\n",
    "The reason lies in a concept known as **overfitting**. Remember, our goal is to make sure the model works well on hiterto unseen data (ie. unlabelled data). If we just throw all the data we have into the train process, then we won't have access to an independent dataset to evaluate the model. Later on, we will see how the test data can be used to assess overfitting. For now, just remember that it is vital that we have an (representative) subset of the labelled data set aside for evaluation. For the purposes of this example, 60,000 images will be used to train and 10,000 images will be used to test:\n",
    "* `Train` -- this is the data we use to fit the model (n=60,000)\n",
    "* `Validate` -- we will not be using this today to tune hyper-parameters\n",
    "* `Test` -- this is the data we use to determine the model performance (n=10,000)\n",
    "\n",
    "**Hyperparameter** = parameters the user (you!) choose beforehand (as opposed to parameters the model learns from the data, such as the weights of a neural network) \n",
    "\n",
    "e.g. the batch number, the number of layers in a neural network, the width of each layer etc.\n",
    "\n",
    "*How do you choose?*  \n",
    "Normally you would have a bunch of values to choose from (eg. `batch_size = {256, 512, 1024}`). Then using these values, you would train your neural network, and apply the model to your validate set. The hyperparameter which gives the best model performance has the best hyperparameters. The overall problem of choosing hyperparameters is called **hyperparameter tuning** and this approach just described is called **grid search**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc21d3-9baa-4185-9c2d-a97df9924c8d",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='#F89536'> **Discussion:** </font> \n",
    "What's the problem with grid search?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3acd82-c9df-42b1-93b5-5cb6b97155d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *DEPRECATE BELOW*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec91d5-3e6e-46e7-9c4d-aee6301af559",
   "metadata": {},
   "source": [
    "### ‚¨áÔ∏è Input\n",
    "\n",
    "The function `load_data_fashion_mnist` takes in two input arguments:\n",
    "* `batch_size`: The number of examples we use to train before each 'update' of the weight. We will discuss this further in Step 5. For now, all we need to know is there is a general trade off:\n",
    "    * Small batch size: Less stable training (since our weight update is reliant on very few examples)\n",
    "    * Large batch size: Tends to be a bit slower and requires a bit more memory.\n",
    "* and `n_workers`: The number of computer (CPU or GPU) cores engaged in reading the data.\n",
    "\n",
    "The recommended argument will be different depending on whether you are running this on mybinder or your own machine.\n",
    "\n",
    "#### For mybinder users (Default)\n",
    "Due to limited resources, these values seem to provide a stable outcome:\n",
    "* `batch_size = 1024`\n",
    "* `n_workers = 0`\n",
    "\n",
    "#### For those on their own machines\n",
    "You should have more resources than mybinder, we recommend the following:\n",
    "* `batch_size = 256`\n",
    "* `n_workers = 4`\n",
    "\n",
    "Note if you are using GPU acceleration, you may choose to use more worker nodes. See Appendix for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267cd51-0f30-4bff-906e-3e46931be891",
   "metadata": {},
   "source": [
    "### üíâ Types of `import`\n",
    "*THIS IS TOO VERBOSE*  \n",
    "You might notices that we are importing libraries in various ways. We need to remind ourselves of the different ways of importing.\n",
    "\n",
    "1. Importing the entire module  \n",
    "This makes the entire package available for use to the user. So if we run `import numpy`, we can access resources inside the `numpy` library by prefixing our desired function with the package name: for example `numpy.array()`.\n",
    "\n",
    "2. Importing and renaming  \n",
    "Sometimes the package names are really long, so we don't fancy writing it out every time. In that case, we can rename it `import numpy as np` so that when we call it again, the prefix doesn't have to be as long: for example `np.array()`.\n",
    "\n",
    "3. Importing sub-modules/resources  \n",
    "If there are resources inside packages that are really useful to us, we can specifically import those. For example `from numpy import array`. In this case, we do not have to prefix it at all: for example `array()`.\n",
    "\n",
    "4. Importing the entire module without prefix  \n",
    "There is a way to import all of a package's resources without having to prefix. This is by using the wildcard `*`, `from numpy import *` will make **all** resources available without prefixing. ***NEVER EVER EVER DO THIS.*** The problem with this method is that often packages have very generic names for their functions (eg. `array()`. This might conflict with pre-existing functions and this will end very badly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c03e-5765-4314-8c26-82e4a4f1ca42",
   "metadata": {},
   "source": [
    "---\n",
    "#### <font color='red'> A Lesson on Data Structures `DataLoader` vs. `DataSet` </font>\n",
    "PyTorch natively provides two data structures to work with. `DataLoader` and `DataSet`. Here's a bit of comparison:\n",
    "\n",
    "| `DataSet` | `DataLoader` |\n",
    "| --- | --- |\n",
    "| Typical dataset object (like a table) | An iterator object |\n",
    "| Reads in all the data at once and stores in memory | Reads in data only when the function is called |\n",
    "| Good for smaller datasets | Good for larger datasets |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83bfb6-f00f-41e8-9766-3aa5cfa7861b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Weird, it's not printing out the mini-batch, but rather some weird thing called a DataLoader object. There's actually a really good reason for this! If we are dealing with big datasets, reading the data in as a table would mean we need to store the entire thing in memory (RAM) - this can be up to 10-20GB which can cripple a normal computer! Instead we use the DataLoader object to read in the dataset as we need it (and discarding it otherwise). This might be a bit slower, but far more manageable in terms of memory resources. This means there's a very particular way to extract a single example, which we will attempt to do below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1919da-70f7-4174-b168-6aac3bea2c02",
   "metadata": {},
   "source": [
    "### ‚¨ÜÔ∏è Output\n",
    "\n",
    "For this particular example, we didn't have to explicitly split the data into train and test set, it came pre-split:\n",
    "* 60,000 images into `train_iter`\n",
    "* 10,000 images into `test_iter`\n",
    "\n",
    "If we try to view examples of these images, we encounter something strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e63f13-81e9-42e5-bf0c-0ff73f39ba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000014AFB8FC970>\n"
     ]
    }
   ],
   "source": [
    "print(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7538a-741f-491d-9824-bc9a5bd139ae",
   "metadata": {},
   "source": [
    "This is weird, we *should* get something like the image of the boot above. Or perhaps even some computerised version of it (like a matrix). Now, there IS a way to do that, but the reason why it doesn't is quite interesting. If you would like to know, read on. Otherwise, skip down a few cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e99a5-4e06-445c-94f8-3960322b5cd7",
   "metadata": {},
   "source": [
    "<font color='#F89536'> **Discussion:** </font> In the code above, we used `batch_no + 1`. Why is the `+ 1` necessary? What does it add?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
