{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2ccb11-d6e5-4c0b-b430-3330334eaaeb",
   "metadata": {},
   "source": [
    "# 💪 Part 3: Building and Training Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b5272d7-5eb3-4585-ab66-1415dbe7ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from helper import helper\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c91d0-748f-4c58-9143-7f64bcaff57f",
   "metadata": {},
   "source": [
    "## ⚗️ The Data Science Pipleine \n",
    "*This section will be repeated in both Part 2 and Part 3*\n",
    "\n",
    "> What on earth is data science?! -- George Washington (probably not)\n",
    "\n",
    "Seriously though, nowadays, in such a data-rich world, data science has become the new buzzword, the new cool kid in the block. But what exactly is it? Unfortunately, no one can really pin down a [rigourous definition](https://hdsr.mitpress.mit.edu/pub/jhy4g6eg/release/7) of data science. At the high level:\n",
    "\n",
    "> Data science is the systematic extraction of novel information from data.\n",
    "\n",
    "Good enough! With this definition, most practitioners can somewhat agree on a pipeline or flow. Here are the steps:\n",
    "1. Identify your problem (What are you trying to do?)\n",
    "2. Obtain your data (What resource do we have to work with?)\n",
    "3. Explore your data (What does our data actually look like?)\n",
    "4. Prepare your data (How do we clean/wrangle our data to make it ingestible?)\n",
    "5. Model your data (How do we automate the process of drawing out insights?)\n",
    "6. Evaluate your model (How good are our predictions?)\n",
    "7. Deploy your model (How can the wider-user base access these insights?)\n",
    "\n",
    "The 7th step is out-of-scope for this workshop, but we well be exploring the other steps to varying degrees:\n",
    "* Steps 1-4 will be explored in Part 2.\n",
    "* Steps 5-6 will be explored in Part 3 and 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d477b-c2b9-4f3f-a07d-75e893797252",
   "metadata": {},
   "source": [
    "## 🧢 Recap\n",
    "Let's review what we have done so far:\n",
    "\n",
    "|Pipeline | Our Problem |\n",
    "|---| --- |\n",
    "|1. Identify Your Problem | Classify images of items of clothing |\n",
    "|2. Obtain Your Data | 70,000 labelled images (10 different types) of clothes |\n",
    "|3. Explore Your Data | Class distribution perfectly equal across classes |\n",
    "|4. Preare Your Data | Split 70,000 into 60,000 train and 10,000 test set |\n",
    "\n",
    "Note that we didn't have to do too much cleaning because the data we have is close to *perfect* in many regards. For further details about intricacies of this process, this excellent [textbook](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) provides all the nitty gritty detail. \n",
    "\n",
    "We need to re-run the data reading part of the tutorial from the last notebook. Please set your variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb212e9-9bf7-4713-ad38-ef36a2825bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the function without running it\n",
    "def load_data_fashion_mnist(batch_size, n_workers):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))\n",
    "\n",
    "# Then execute the function here\n",
    "batch_size = 1024  # Set to 256 on your own device\n",
    "n_workers = 0      # Set to 4 on your own device\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcea701-25a1-462f-a050-7ab5d966df99",
   "metadata": {},
   "source": [
    "## Step 5A: Setting Up Your Model\n",
    "So we have our data fully prepared and ready to train. What is our model going to look like? Recall that neural networks are made of building blocks called 'Perceptrons'. Put the perceptrons in a proper way and we'll get ourselves a nice neural network. We won't go into the intricate details of how to determine the architecture. Instead we will use work that has already been done.\n",
    "\n",
    "### 🪟 Convolutional Neural Network\n",
    "Recall that for image data, a typical/simple neural network such as the multi-layer perceptron (or dense neural network or fully-connected neural network) can be okay, but is usually not powerful enough to capture the information in pictures. Instead we use **convolutional** neural networks. The mathematical concept of [convolution](https://en.wikipedia.org/wiki/Convolution#Visual_explanation) can take a bit of time to get used to, but instead the best way to think about it is using a window to 'look' at the image chunks at a time to process it.\n",
    "\n",
    "![](../images/convolution.gif)  \n",
    "[source](https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Full_padding_no_strides.gif)\n",
    "\n",
    "### LeNet\n",
    "The particular convolutional neural network architecture we will use is called the LeNet. It was one of the first successful neural network architectures to be concieved by Yann LeCun as he worked at Bell Laboratory. Here is the [original paper](https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition) if you are interested. In this instance, we will be building a slightly adapted version that the d2l.ai textbook outlines in [this chapter](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html). (The key difference is that we will be dropping the Gaussian activation function in the final layer).\n",
    "\n",
    "We will be using the two diagrams below to construct our neural network:\n",
    "\n",
    "![lenet](../images/lenet.svg)\n",
    "\n",
    "**Figure 1:** The architecture of LeNet. ([source](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html))\n",
    "\n",
    "![lenetsimple](../images/lenet-vert.svg)\n",
    "\n",
    "**Figure 2:** Compact version of the architecture of LeNet. ([source](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html))\n",
    "\n",
    "#### Convolution Layers\n",
    "Recall from the presentation that the convolution layer takes in 4 hyperparameters. The first convolution layer from Figure 2 provides us with all the key answers to these:\n",
    "\n",
    "| Hyperparameter | Description | Value |\n",
    "| --- | --- | --- |\n",
    "| Kernel Size | Size of window | 5 (5x5) | \n",
    "| Output Layers | Number of filters | 6 |\n",
    "| Padding | Size of the '0' ring | 2 |\n",
    "| Stride | How far to slide the window | 1 |\n",
    "\n",
    "Note that if the stride is not stated, the default value of 1 is used.\n",
    "\n",
    "<font color='#F89536'> **Discussion:** </font> If the input image is 28x28, what are the output dimensions of the layer? (Hint: With pad-2, the image will be a 30x30 image. How many strides (of 1) can the window move horizontally before reaching the right-hand side?)\n",
    "\n",
    "Since our images are black and white, there is only a single input channel. In code, this looks like:  \n",
    "`nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size=5, padding=2)`.\n",
    "\n",
    "**Note:** If you want to get a more conceptual understanding of what is happening, [this Stanford University course](https://cs231n.github.io/convolutional-networks/) has an animated figure which you can toggle on and off.\n",
    "\n",
    "<font color='red'>Examples! Maybe discuss [cross-correlation](http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html?highlight=cross%20correlation) also</font>.\n",
    "\n",
    "#### Pooling Layers\n",
    "All pooling in LeNet is average pooling. It takes in two hyperparameters. The first pooling layer (bottom of Figure 2) gives:\n",
    "\n",
    "| Hyperparameter | Description | Value |\n",
    "| --- | --- | --- |\n",
    "| Kernel Size | Size of window | 2 (2x2) | \n",
    "| Stride | How far to slide the window | 2 |\n",
    "\n",
    "Putting this together, the code becomes: `nn.AvgPool2d(kernel_size=2, stride=2)`.\n",
    "\n",
    "\n",
    "#### Linear/Dense Layer\n",
    "Let's look at the FC(120) layer in Figure 2. The input is 16 layers of 5x5 images. How many pixels is that? $16 \\times 5 \\times 5 = 400$. We squish each pixel into a fully connected layer with 120 as the output.\n",
    "\n",
    "Putting this together, the code becomes: `nn.Linear(in_features = 16 * 5 * 5, out_features = 120)`.\n",
    "\n",
    "<font color='red'>Examples!</font>.\n",
    "\n",
    "\n",
    "#### Tying Loose Ends\n",
    "Between each layer, we will use a sigmoid function `nn.Sigmoid()` as our activation function. Note we do not need to put activation functions after pooling layers, only after convolutional and linear layers. \n",
    "\n",
    "To convert from a 2D image representation to a 1D linear representation, we use the `nn.Flatten()` function. Try filling out the `?` below according to the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9e053f-9f06-432c-a936-c78942cdf23f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13272/2565157279.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Brian\\AppData\\Local\\Temp/ipykernel_13272/2565157279.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    nn.Conv2d(?, ?, kernel_size=?), ?,\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialise LeNet Architecture\n",
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(?, ?, kernel_size=?), ?,\n",
    "                    ?, nn.Flatten(),\n",
    "                    nn.Linear(16 * 5 * 5, 120), ?,\n",
    "                    ?, ?, \n",
    "                    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43330c5b-398f-428d-b93f-df2849cb84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LeNet Architecture (ans)\n",
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "                    nn.Linear(120, 84), nn.Sigmoid(), \n",
    "                    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576944d3-f49e-490a-b352-4540c4429dd9",
   "metadata": {},
   "source": [
    "Let's have a look at what each layer looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f18e770-fe1a-4b36-b30b-c4fb133a948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:    \t torch.Size([1, 400])\n",
      "Linear output shape:    \t torch.Size([1, 120])\n",
      "Sigmoid output shape:    \t torch.Size([1, 120])\n",
      "Linear output shape:    \t torch.Size([1, 84])\n",
      "Sigmoid output shape:    \t torch.Size([1, 84])\n",
      "Linear output shape:    \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Show layers\n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:    \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75e678-e8ea-4139-9bdb-c6b8cf0b21ea",
   "metadata": {},
   "source": [
    "## Step 5B: Training Your Model\n",
    "\n",
    "### Initialising weights\n",
    "When you set up your neural network `net`, the weights are completely random and non-sensical. There are, however, certain systematic ways of randomly initialising your weights (as weird as that sentence sounds). A sensible starting point is using the [Xavier Uniform](https://pytorch.org/docs/stable/nn.init.html#) distribution as outlined in [this paper](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). No further justification will be given in this tutorial, although initial weight values of neural networks can spark fascinating discussions in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c99d04-b35f-4dfb-9c04-46ad8588df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d: # We will only set the weights from linear and Conv2d layers, since pooling layers do not require this\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights) # the function apply() takes in another function as input! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1e570-18d1-497b-a17d-024da8c8af59",
   "metadata": {},
   "source": [
    "### Setting Hyperparameters\n",
    "There are certain parameters that we allow the data to define (eg. the weights of the neural network). There are other parameters we need to provide to the model (eg. the exact architecture of the neural network). The latter is called **hyperparameters**. We have already set the network architecture, but here are three more points of decision we need to make:\n",
    "\n",
    "| Hyperparameter | Description | Selected Value |\n",
    "| --- | --- | --- |\n",
    "| Learning Rate | How quickly the algorithm converges. Too quick and we might *miss* the optimal weights. Too slow and it will take a long time to run | $0.9$ |\n",
    "| Optimiser | What algorithm do we use to find the optimal weights? | [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) |\n",
    "| Loss Function | How do we measure the *correctness* of our predictions? | [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) |\n",
    "\n",
    "We define these below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83d7b334-b6d2-419e-bdf5-646d115dbb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.9 \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr) \n",
    "loss = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b03103-5f9f-4314-844d-8ed8fce1311e",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "To train the model we must:\n",
    "1. Set the 'mode' of the network to `train`\n",
    "2. Select a single mini-batch to train on\n",
    "3. Conduct a forward pass to make predictions\n",
    "4. Calculate the loss (lack of 'correctness') of these predictions\n",
    "5. Calculate the gradients required for back propagation (according to the loss function)\n",
    "6. Update weights according to gradient descent\n",
    "\n",
    "Each line below corresponds to each of these steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2673e4c-b7e0-4f71-b7c4-90c7323577f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train() # This doesn't actually train, but sets the network on training mode\n",
    "X, y = next(iter(train_iter)) # Pick a single minibatch at random to do the training\n",
    "optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "y_hat = net(X) # Forward pass on the data to make prediction\n",
    "l = loss(y_hat, y) # calculate the loss \n",
    "l.backward() # calculate gradients for back prop\n",
    "optimizer.step() # step forward in optimisation and apply backprop to update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8adee2-55a7-4d6c-ba5b-3162da3e5b7f",
   "metadata": {},
   "source": [
    "🎉🎉🎉 Congratulations! You have trained your first neural network in PyTorch 🎉🎉🎉\n",
    "\n",
    "...well not quite. This model is going to be quite terrible, since we only trained on a small sample of our dataset. In the next part we will look into scaling this procedure up. But first, let's see how we went.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc745c0-3d6a-4b0f-8bb1-4843fcd188ae",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Your Model\n",
    "It's all well and good if you can train a model, but it's pretty useless if you can't see how well it does. Recall that our performance metric is that we want:\n",
    "* Predictions to be correct (Accuracy)\n",
    "* Model to generalise to unseen data (No Overfitting)\n",
    "\n",
    "Thus we should extract both the train accuracy (how well the model runs on the dataset it trained on), and the test accuracy (how well the model runs on unseen/independent data).\n",
    "\n",
    "### Training Accuracy\n",
    "First we calculate the training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "249fcb37-c33f-417e-a534-6a9792d29892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The mini-batch loss is: \t\t\t\t tensor(2604.3523, grad_fn=<MulBackward0>)\n",
      "2. The number of correct training predictions is: \t 93.0\n",
      "3. The number of total training predictions is: \t 1024\n",
      "This means we get a training accuracy of  0.0908203125\n",
      "The average loss for each example is  2.5433127880096436\n"
     ]
    }
   ],
   "source": [
    "loss = l * X.shape[0]\n",
    "n_correct = helper.accuracy(y_hat, y)\n",
    "n_total = X.shape[0] \n",
    "\n",
    "\n",
    "print(\"1. The mini-batch loss is: \\t\\t\\t\\t\", loss)\n",
    "print(\"2. The number of correct training predictions is: \\t\", n_correct)\n",
    "print(\"3. The number of total training predictions is: \\t\", n_total)\n",
    "\n",
    "print(\"This means we get a training accuracy of \", n_correct/n_total)\n",
    "print(\"The average loss for each example is \", float(loss/n_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8df3da-e8c3-4f35-a5d5-4219316ff7f2",
   "metadata": {},
   "source": [
    "### Testing Accuracy\n",
    "Then we calculate the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a6ca85a-e47b-4bdf-b8be-7c8d4d5a1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is:  0.1\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = helper.evaluate_accuracy(net, test_iter)\n",
    "print(\"The testing accuracy is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ba65e-45cd-450d-939c-fdf95ce5b5a1",
   "metadata": {},
   "source": [
    "The train and test accuracy both hover around 10%. That means the model gets the right label about 1 in 10 times. This is no better than randomly picking labels for each image! However, we have only trained over a single mini batch of data. Of course the performance is going to be low. In reality we need to run it over the entire data at least once. Each time we run over the train data once is called an epoch. In the next section we will talk about scaling this up for more training examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
