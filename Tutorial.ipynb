{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d975d00",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Image Recognition\n",
    "\n",
    "<font color='red'>This is the dev branch of the tutorial. Red indicates parts which need to be updated.</font>\n",
    "\n",
    "### Credits\n",
    "This tutorial was heavily borrowed from the [d2l.ia](http://d2l.ai/). Major changes made include:\n",
    "* Simplification and adaptation to cpu-only implementation\n",
    "* Comments and descriptions of the functions\n",
    "\n",
    "Note the sample code in the d2l.ia [repo](https://github.com/d2l-ai/d2l-en) is subject to a modified [MIT License](https://en.wikipedia.org/wiki/MIT_License). See the source [github repo](https://github.com/bbpi2/cnn-pytorch-tutorial) for the licensing of this tutorial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17c440-010f-4191-ad83-3491fe6673d1",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this tutorial, we will be building a Convolutional Neural Network (CNN) as a model to classify images. Pre-requisites:\n",
    "* Basic knowledge of Python \\& Jupyter Notebooks. If you have not used Python before, please refer to the <font color='red'>Primer</font>.\n",
    "* Basic knowledge of CNNs from either the previous presentation or otherwise. For some references, please refer to this [excellent course by Stanford Unviersity](https://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "### 1.1 PyTorch\n",
    "<font color='red'>Need to add primer for why we use libraries</font>.\n",
    "\n",
    "There are many Python libraries which facilitate Deep Learning and Neural Networks. Currently the two most popular are [TensorFlow](https://www.tensorflow.org/) (owned by Google) and [PyTorch](https://pytorch.org/) (owned by Facebook). Both are open-sourced, which means the code is available for all to see and make improvements upon. TensorFlow has a frontend application called [Keras](https://keras.io/) which makes creating a neural network in TensorFlow a little easier at the expense of some flexibility. For pedagogical reason, this tutorial will focus on using PyTorch. After gaining some familiarity with PyTorch, learning TensorFlow with Keras should be a simpler exercise.\n",
    "\n",
    "### 1.2 Key Libraries\n",
    "Here are the key libraries we will be using:\n",
    "\n",
    "| Library | Description |\n",
    "| --- | --- |\n",
    "| `torch` | This is the core `PyTorch` library doing most of the heavy lifting |\n",
    "| `torchvision` | This is how we will extract the data (if needed), and apply some cleaning to images |\n",
    "| `random` | This library provides random number generators. This is important if we want to sample images from our dataset | \n",
    "| `matplotlib.pyplot` | This is a very popular plotting library |\n",
    "| `time` | This has functions which allows us to keep track of how long the code takes to run |\n",
    "| `IPython` | This libraries allows us to access some of the options of these notebooks |\n",
    "| `numpy` | This is a very popular data manipulation library |\n",
    "\n",
    "The follow code imports the libraries into our current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ff1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "random.seed(2021) # We set a seed to ensure our samples will be the same every time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbbfab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# helper = reload(helper)\n",
    "# from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e71401",
   "metadata": {},
   "source": [
    "# <font color='red'>STOP!!!!!!!!!!!! Run the two helper function cells at the bottom before proceeding</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb234b3f",
   "metadata": {},
   "source": [
    "## 2. Data Science Pipeline\n",
    "Before we jump into implementing the fancy neural networks, it is important for us to understand the data science process. Neural networks are but a tool that form part of an overall exercise of understanding and analysing data. The overarching goal should be to draw novel insights from data (often about the real world), and them subsequently making inferences (decisions) about them. There is no clear concensus about what the analysis process should really be like, at its crux, here are some key elements:\n",
    "\n",
    "1. Identify your problem (what exactly are you trying to achieve?)\n",
    "2. Obtain your data (what is the raw resource we have to work with?)\n",
    "3. Explore your data (often, we're not the ones who have collected it, so we need to make sure to understand it)\n",
    "4. Prepare/Clean/Wrangle your data (this involves many many things, from dealing with missingness, to standardising)\n",
    "5. Model your data (this is where neural networks can come in)\n",
    "6. Evaluate your models (how good are your predictions? Is it biased? Can we do better? How much would improvements cost?)\n",
    "7. Deploy your model (this is a whole other rabbit hole that we will only touch upon).\n",
    "\n",
    "### 2.1 Identify Your Problem\n",
    "For the purpose of this tutorial, let's consider a hypothetical. Your boss comes up to you and tells you that you will be given lots and lots of black and white pictures of clothing. Your job is to identify what type of clothing each image pertains to. Of course, you can just look at the image, and make that decision. But the problem with this approach is that it is not **scalable**. It's fine for maybe 100 or 1,000 images, but what if you had 1,000,000? What if you needed to sort through all 1 million in a day? I'm sure after 10,000 you'd already be tired and your decision making won't be as sharp as the first 10k, so you lose even a bit of consistency. This is the perfect problem for machine learning because it is:\n",
    "* Scalable -- once we have a *good enough* model, we can just run it over however many examples our heart desires (limited only by computational resources).\n",
    "* Consistent -- in the long term, the data-driven decisions of the algorithm will be more consistent than a group of humans (or even one very tired human)\n",
    "\n",
    "### 2.2 Obtain Your Data\n",
    "Lucky for you, your boss mentioned that previously they hired some poor soul who managed to sort through 70,000 of these images already. For example, the following image is a boot:\n",
    "\n",
    "![Boot](./images/boot.png)\n",
    "\n",
    "You do a bit of data exploration and you find the following facts:\n",
    "* There are 70,000 labelled images.\n",
    "* Each image is an item of clothing.\n",
    "* Each image is a 28x28 sized image (784 pixels in total).\n",
    "* Each pixel is black and white, and has a value between 0 and 255 indicating the brightness of the pixel\n",
    "* There are a total of 10 types of items of clothing.\n",
    "\n",
    "| Label | Type of Clothing |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat | \n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle Boot |\n",
    "\n",
    "In fact, the dataset we are working with is called the [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist). Below we will define a function to extract the data in a particular way. There is no need to fully understand what it's doing, only what inputs and outputs are associated with this function.\n",
    "\n",
    "**Input:**\n",
    "* `batch_size` - the number of examples (images) we run for each mini-batch (we will come back to this later)\n",
    "    * If you are running this on mybinder, set `batch_size = 1024` or higher\n",
    "    * If you are running this on your own machine, set `batch_size = 256` or higher\n",
    "* `n_workers` - how many computer cores we want to employ for reading the data \n",
    "    * If you are running this on mybinder, set `n_workers = 0`\n",
    "    * If you are running this on your own machine with CPU only, set `n_worker = 4`\n",
    "    * If you are running this on your own machine with GPU also, set `n_worker = 4n`, where `n` is the number of cores in your GPU (eg. if `n=4` then `n_workers=16`)\n",
    "\n",
    "*If you are running this on a GPU, there is a post-script on how to do this. We recommend running it on CPU first then trying GPU as a challenge.*\n",
    "\n",
    "**Output:**\n",
    "* `train_iter` - this is a train set with 60,000 units\n",
    "* `test_iter` - this is a test set with 10,000 units\n",
    "\n",
    "*We will come back to the idea of train/test split later on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a240c6f-14e6-4efd-852b-b29a0acbde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, n_workers=0):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7692241d-8342-4782-a762-a2a786633c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size) # n=60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee74f9b-f2b5-4580-93b1-a1d5f36a8d3d",
   "metadata": {},
   "source": [
    "At this point, we will have two datasets: `train_iter` and `test_iter` which total 70,000 labelled images. If you are familiar with Python, you may want to try to have a look at a single example from these datasets. This can be done with the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f444d96-1b11-4595-b464-56b174efff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA85D42CA0>\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09e6d2-321b-4cca-b9f6-c93889ca9b53",
   "metadata": {},
   "source": [
    "Hmm this is weird, why does it not show the dataset? Maybe if we try to extract a single example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bae866e-ba62-4b30-b98d-7ea5acee0fdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9956/1295936104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(train_iter[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef94e2-8975-4ca8-8c4c-d013276ddb3a",
   "metadata": {},
   "source": [
    "Error! That's strange indeed. Why is this the case? The answer is quite complicated. If you are interested, scroll down to the appendix to find out more. For now, just know that this is the way to extract a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a8236d-c432-4b5e-8772-7ba819a08997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches we have extracted are: 3\n",
      "The number of examples in the batch we selected are: 1024  and  1024 . Note these two values should be equal.\n",
      "The shape of the predictor torch.Size([1, 28, 28])\n",
      "The shape of the label torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# First we extract a few random batches (say 3), each of which will have up to your set batch_size (eg. 1024 for mybinder)\n",
    "sampled_batches = random.sample(list(train_iter), 3)\n",
    "print(\"The number of batches we have extracted are:\", len(sampled_batches))\n",
    "\n",
    "# Second we select a single batch to look at, let's say the 3rd one\n",
    "batch_no = 2\n",
    "## 0 denotes the predictors\n",
    "## 1 denotes the labels\n",
    "predictor = sampled_batches[batch_no][0]\n",
    "label = sampled_batches[batch_no][1]\n",
    "print(\"The number of examples in the batch we selected are:\" , len(predictor), \" and \", len(label), \". Note these two values should be equal.\")\n",
    "\n",
    "# Third, we select a single example in the batch, let's say the 100th one\n",
    "example_no = 99\n",
    "single_predictor = predictor[example_no]\n",
    "single_label = label[example_no]\n",
    "print(\"The shape of the predictor\", single_predictor.shape)\n",
    "print(\"The shape of the label\", single_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd032c9-432d-40e9-a449-d8d6b4a85365",
   "metadata": {},
   "source": [
    "Let's go into the last two lines in a bit of detail. \n",
    "\n",
    "The predictor has the shape `[1, 28, 28]`. The first number indicates the channel. Since we are dealing with black and white, we only really need 1. For colour, this should be 3 (RGB). The 2nd and 3rd number are the width and the height of the image.\n",
    "\n",
    "The label has the shape `[]`, which means it is just a number.\n",
    "\n",
    "Now finally, we can print the predictor (example) and the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c4cf48-ee20-46c2-9ee1-cb9f8a4c4d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ8klEQVR4nO3dXYxU93kG8OfZ3dlP8Jo1H+ZjDTbFdh03xtEGtyWubFmJiBUJR5WrcBFRySq5sKWkykUt9yK+tKomVi4qS6RGIW3iNFXimgsrBlFLNHLjsjgYcKgLxnwsbHcxH96F/ZrZeXuxx9Uu7HnPsvMJ7/OT0OzOO2fm3WGfPTPzP//zp5lBRG59DbVuQESqQ2EXCUJhFwlCYRcJQmEXCaKpmg/WzBZrRUc1H/Lm0N7qlic6G906SxhQYSHrBn7ZMnYXTWPpzVnGfWdpuDxS2h3cgsZwFRM2PuszW1LYSW4C8EMAjQD+0cxe8m7fig48widKechbEj/3oFs/vek2f/tieq0hI8wtl/y/FMWM35B8h5/Yrg/TGzD/b1imtn/7r9Lu4Bb0ru1Nrc37ZTzJRgD/AOCrAB4AsIXkA/O9PxGprFLes28AcNzMTpjZBICfA9hcnrZEpNxKCftKAGemfd+XXDcDyW0ke0n25jFewsOJSClKCftsb9auewNoZtvNrMfMenJoKeHhRKQUpYS9D0D3tO9XAThXWjsiUimlhH0/gHUk7ybZDOAbAHaVpy0RKbd5D72ZWYHkcwDewtTQ2w4z+6BsnQVy5sv+0NojXzvs1k8Od6XWlrRdcbd9dNFxt35wuNutr2q75Nb/+e1HU2sr9mnGZTWVNM5uZm8CeLNMvYhIBelwWZEgFHaRIBR2kSAUdpEgFHaRIBR2kSCqOp9dZpc1J/z0lUVu/cLV9tTawKcL/W3H/PMLfNy3xK0vW3bZrRcXpk9xPf9Qs7tt6wW3jI4v/pFbt/3+8QnRaM8uEoTCLhKEwi4ShMIuEoTCLhKEwi4ShIbe6sDoH/in6zp94Lqzfc1QbEmfKsqlY+62Vyf84S98mnPLAw2dbp1N6ae+tfv96bf5pkm3jv/wyzKT9uwiQSjsIkEo7CJBKOwiQSjsIkEo7CJBKOwiQWicvQoa70g/1TMALPi9v1JOw4R//8NfSB+nb8xYz7m50R/Lbl3pj4U3ZYyF53+XPj2345zf2+X7/HrD8IBbzxilD0d7dpEgFHaRIBR2kSAUdpEgFHaRIBR2kSAUdpEgNM5eBYV7/WWPR1amz/mei8Zc+vaFfKO77f9e8k81/fg9x9z6oQsr3Pr4YPpY+W0f+wcQjN/uH39QvN0/DbbMVFLYSZ4EMIyp4xcKZtZTjqZEpPzKsWd/3Mw+KcP9iEgF6T27SBClht0A7CZ5gOS22W5AchvJXpK9efjnWhORyin1ZfxGMztHcimAPST/28z2Tb+BmW0HsB0AbmOXP7NBRCqmpD27mZ1LLgcBvA5gQzmaEpHym3fYSXaQXPjZ1wC+AuBIuRoTkfIq5WX8MgCvk/zsfn5mZr8uS1e3mOE1bW69cdmoW58c8LcvjKX/NzZe9P+LF97nz1df0fKpW39n7G63jg6mlhom/Xd1TSP+XbPgH5+g94wzzTvsZnYCwENl7EVEKkhDbyJBKOwiQSjsIkEo7CJBKOwiQWiKaxWMd6YPPwHZQ2tNV/3tJ9rS68VmfwAq1+gPXw0VWt36493+FNi3+cXUmjX4PxeLfu8cz7t1mUl7dpEgFHaRIBR2kSAUdpEgFHaRIBR2kSAUdpEgNM5eBcWcP57cfNmvF9ozxpvH0k8Xvep+f1njsZ/d6dYPH1/g1v/1X15x648sSB9nH12cc7ctOMcPAACv+FODZSbt2UWCUNhFglDYRYJQ2EWCUNhFglDYRYJQ2EWC0Dh7FTBjRWYW/HpDxqpZ5qzKvLjNP1X02Dv+OPzkR6fcemeDPxe/5VJ6rXEi4/iBjHNB24S/5LPMpD27SBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAaZ6+ChkLGeHLRn7dd9Kd9A8vHUkvnrnS6m3YeO55x575fj7S49fzC9Npki/9zt37iP2+Tg+fdusyUuWcnuYPkIMkj067rIrmH5LHkclFl2xSRUs3lZfyPAWy65rrnAew1s3UA9ibfi0gdywy7me0DcPGaqzcD2Jl8vRPAU+VtS0TKbb4f0C0zs34ASC6Xpt2Q5DaSvSR788g4yFtEKqbin8ab2XYz6zGznhz8D3NEpHLmG/YBkssBILkcLF9LIlIJ8w37LgBbk6+3AnijPO2ISKVkjrOTfA3AYwAWk+wD8D0ALwH4BclnAJwG8HQlm7zZZc1n9+ajA4DlMsabx9PvYPDEHe62nShtnH3f8H1uvdCR3rv5w+xoGvd/7oY2fy59cWTEf4BgMsNuZltSSk+UuRcRqSAdLisShMIuEoTCLhKEwi4ShMIuEoSmuFZB85A/hDS82t++6WrGks+r0w9Dzr1f2aMW/73/XrdeaEv/2YtN/s+V7/Afu6HLn2ypobeZtGcXCUJhFwlCYRcJQmEXCUJhFwlCYRcJQmEXCULj7FXQPugvLXy+yR8Ln2z358iuXfRpas3eb3a3LdXQO6lnJJt6/HvSf/bmq/7c3tFmf19U6Dvr1mUm7dlFglDYRYJQ2EWCUNhFglDYRYJQ2EWCUNhFgtA4exU0jhYybpEx5zzjlMsfD6SfLvrufMZ5rEt011vDbv34X6ePpU/m/F8/y/jtZM4/hsDy/vEN0WjPLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKExtmroNCR82+Q9SfXP+08updcSq01n5p0t806AiCL7T/s1jesTj+3+7H2+/37zji+oHHFMrdeOHXGv4NgMvfsJHeQHCR5ZNp1L5I8S/Jg8u/JyrYpIqWay8v4HwPYNMv1L5vZ+uTfm+VtS0TKLTPsZrYPwMUq9CIiFVTKB3TPkTyUvMxPfWNGchvJXpK9eaSvSSYilTXfsL8CYC2A9QD6AXw/7YZmtt3MesysJ5c14UNEKmZeYTezATObNLMigB8B2FDetkSk3OYVdpLLp337dQBH0m4rIvUhc5yd5GsAHgOwmGQfgO8BeIzkekyNAJ8E8K3KtXjza/3ovFsv5la4dWv3x8qvTqTP624+c8zdttLeO7sqtcZufyC9bdA/wKCw3F+fHRpnnyEz7Ga2ZZarX61ALyJSQTpcViQIhV0kCIVdJAiFXSQIhV0kCE1xrYLCydNuPTeUPjwFAOO3+fd/8fKC1FpXi3/Uoo1nHMLMjHmm5g+PTYw603vXjrnbdp7wTxU9vLrdrS/8rVsOR3t2kSAUdpEgFHaRIBR2kSAUdpEgFHaRIBR2kSA0zl4Hcv6qx5kn88o1p58QOv+lB91tm/Ye8O+cGfsD86ffWiF9+9bb/J9sMucfIzC62D8GYKFbjUd7dpEgFHaRIBR2kSAUdpEgFHaRIBR2kSAUdpEgNM5eB4r+tG3k2vNu3ZtSfu5Rf6z6rr3+Y6Poj6NnYVMxtbZ2ySfutkc/3+HWF36cMddeZtCeXSQIhV0kCIVdJAiFXSQIhV0kCIVdJAiFXSQIjbPXgbYB/9zrHRtH3PrQ1dbUWtNDl+fTUtnYaPqv2LJWfyL/sRX+z93yO38cXmbK3LOT7Cb5NsmjJD8g+e3k+i6Se0geSy4zFssWkVqay8v4AoDvmtkfAvhjAM+SfADA8wD2mtk6AHuT70WkTmWG3cz6zey95OthAEcBrASwGcDO5GY7ATxVoR5FpAxu6AM6kmsAPAzgXQDLzKwfmPqDAGBpyjbbSPaS7M1nnk1NRCplzmEnuQDALwF8x8yG5rqdmW03sx4z68nBn5QhIpUzp7CTzGEq6D81s18lVw+QXJ7UlwMYrEyLIlIOmUNvJAngVQBHzewH00q7AGwF8FJy+UZFOgxg2e4zbv3Cn6cPrQH+qsrjY86SyQCY8+fXWn7CrWdyprgW4U9RzeX86bW5kfT7luvNZZx9I4BvAjhM8mBy3QuYCvkvSD4D4DSApyvSoYiURWbYzew3QOqf4CfK246IVIoOlxUJQmEXCUJhFwlCYRcJQmEXCUJTXOtA4UyfW1/acadbHxpJH4dvafVPQz3x+Ofdem53r1vP0tg2/1NRZx0j0JD3pwbLTNqziwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShcfabwOEPu936nd0XU2vjef+/uP9P/Pnsd+12y5maW9LH+Ttzo+62bPDnqxfatK+6EXq2RIJQ2EWCUNhFglDYRYJQ2EWCUNhFglDYRYLQOHs1eCd2BwDLWLL5hD+vu2VN+pzxq+P+OPrEosqee310OH0VoNFJ/+fqaPPPWZ9v15LNN0J7dpEgFHaRIBR2kSAUdpEgFHaRIBR2kSAUdpEg5rI+ezeAnwC4E0ARwHYz+yHJFwH8FYDzyU1fMLM3K9XoTY0Zf1PNP7f6wlP+WHh7Ln08eqghfZwbAIoLCm69VI0X0sfSz47c7m47NNzm1jv9QwjkGnM5qKYA4Ltm9h7JhQAOkNyT1F42s7+vXHsiUi5zWZ+9H0B/8vUwyaMAVla6MREprxt6z05yDYCHAbybXPUcyUMkd5BclLLNNpK9JHvzGC+tWxGZtzmHneQCAL8E8B0zGwLwCoC1ANZjas///dm2M7PtZtZjZj05+O8fRaRy5hR2kjlMBf2nZvYrADCzATObNLMigB8B2FC5NkWkVJlhJ0kArwI4amY/mHb98mk3+zqAI+VvT0TKZS6fxm8E8E0Ah0keTK57AcAWkusBGICTAL5Vgf5uCWzwp7haxizTrv8859Y7nk3/LKSzzZ9Guube9NNQA8BVt5qtuDj9VNJdLf69P7z6jFs/s3vdvHqKai6fxv8GwGy/rRpTF7mJ6Ag6kSAUdpEgFHaRIBR2kSAUdpEgFHaRIHQq6SqwSX8Ka5bCydNu/chbf5pezBjDb957JePRz2fUffe9PJJae/fpz7nbdpz173vpawfcun+C7ni0ZxcJQmEXCUJhFwlCYRcJQmEXCUJhFwlCYRcJgpaxXHBZH4w8D+DUtKsWA/ikag3cmHrtrV77AtTbfJWzt9VmtmS2QlXDft2Dk71m1lOzBhz12lu99gWot/mqVm96GS8ShMIuEkStw769xo/vqdfe6rUvQL3NV1V6q+l7dhGpnlrv2UWkShR2kSBqEnaSm0h+SPI4yedr0UMakidJHiZ5kGRvjXvZQXKQ5JFp13WR3EPyWHI56xp7NertRZJnk+fuIMkna9RbN8m3SR4l+QHJbyfX1/S5c/qqyvNW9ffsJBsB/A+ALwPoA7AfwBYz+31VG0lB8iSAHjOr+QEYJP8MwBUAPzGzB5Pr/g7ARTN7KflDucjM/qZOensRwJVaL+OdrFa0fPoy4wCeAvCXqOFz5/T1F6jC81aLPfsGAMfN7ISZTQD4OYDNNeij7pnZPgDXLtmyGcDO5OudmPplqbqU3uqCmfWb2XvJ18MAPltmvKbPndNXVdQi7CsBTF/Xpw/1td67AdhN8gDJbbVuZhbLzKwfmPrlAbC0xv1cK3MZ72q6Zpnxunnu5rP8ealqEfbZlpKqp/G/jWb2BQBfBfBs8nJV5mZOy3hXyyzLjNeF+S5/XqpahL0PQPe071cB8FcurCIzO5dcDgJ4HfW3FPXAZyvoJpeDNe7n/9XTMt6zLTOOOnjuarn8eS3Cvh/AOpJ3k2wG8A0Au2rQx3VIdiQfnIBkB4CvoP6Wot4FYGvy9VYAb9SwlxnqZRnvtGXGUePnrubLn5tZ1f8BeBJTn8h/BOBva9FDSl/3AHg/+fdBrXsD8BqmXtblMfWK6BkAdwDYC+BYctlVR739E4DDAA5hKljLa9TblzD11vAQgIPJvydr/dw5fVXledPhsiJB6Ag6kSAUdpEgFHaRIBR2kSAUdpEgFHaRIBR2kSD+D1k99QUpkvPLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows an image of:  1\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(single_predictor[0]) # print only the one channel of BW\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows an image of: \", int(single_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b2247-1f5b-451d-a97e-419cabad3425",
   "metadata": {},
   "source": [
    "### 2.3 Explore Your Data\n",
    "The main thing we will be doing in this section is looking at the class distribution of the examples. That is, how spread out is each type of clothing? Is it representative? etc. We will not be looking at the code in detail, so we have written a function to do this exploration for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c4e2e6-6d7e-4af5-b8c2-98bc0c87661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Class 0 has 6000 images\n",
      "Class 1 has 6000 images\n",
      "Class 2 has 6000 images\n",
      "Class 3 has 6000 images\n",
      "Class 4 has 6000 images\n",
      "Class 5 has 6000 images\n",
      "Class 6 has 6000 images\n",
      "Class 7 has 6000 images\n",
      "Class 8 has 6000 images\n",
      "Class 9 has 6000 images\n",
      "Test Data\n",
      "Class 0 has 1000 images\n",
      "Class 1 has 1000 images\n",
      "Class 2 has 1000 images\n",
      "Class 3 has 1000 images\n",
      "Class 4 has 1000 images\n",
      "Class 5 has 1000 images\n",
      "Class 6 has 1000 images\n",
      "Class 7 has 1000 images\n",
      "Class 8 has 1000 images\n",
      "Class 9 has 1000 images\n"
     ]
    }
   ],
   "source": [
    "def data_explore(data_iter):\n",
    "    class_count = Accumulator(10)\n",
    "    for i, (X, y) in enumerate(data_iter):\n",
    "        current_counter = torch.bincount(y)\n",
    "        class_count.add(current_counter[0], current_counter[1], current_counter[2], current_counter[3], current_counter[4],\n",
    "                  current_counter[5], current_counter[6], current_counter[7], current_counter[8], current_counter[9])\n",
    "    for i in range(10):\n",
    "        print(\"Class\", i, \"has\", int(class_count.__getitem__(i)), \"images\")\n",
    "        \n",
    "    return\n",
    "        \n",
    "print(\"Train Data:\")\n",
    "data_explore(train_iter)\n",
    "print(\"Test Data\")\n",
    "data_explore(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6351d4-a037-42ec-aec8-c4f875db2a23",
   "metadata": {},
   "source": [
    "The class distribution is perfectly balanced as you would expect from a nice and clean dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b76c6-95b3-4918-b8ab-fdc3a4c4036f",
   "metadata": {},
   "source": [
    "### 2.4 Prepare Your Data\n",
    "This was something we have previously alluded to, but we have 70,000 labelled examples. Perfect! Do we throw them all into the training/fitting process of the model?\n",
    "\n",
    "The answer is **NO**. Why is that though? Doesn't more data = better model?\n",
    "\n",
    "![yesbutno](./images/yesbutno.jpg)\n",
    "\n",
    "The reason lies in a concept known as **overfitting**. Remember, our goal is to make sure the model works well on hiterto unseen data (ie. unlabelled data). If we just throw all the data we have into the train process, then we won't have access to an independent dataset to evaluate the model. Later on, we will see how the test data can be used to assess overfitting. For now, just remember that it is vital that we have an (representative) subset of the labelled data set aside for evaluation. For the purposes of this example, 60,000 images will be used to train and 10,000 images will be used to test:\n",
    "* `Train` -- this is the data we use to fit the model (n=60,000)\n",
    "* `Validate` -- we will not be using this today to tune hyper-parameters\n",
    "* `Test` -- this is the data we use to determine the model performance (n=10,000)\n",
    "\n",
    "#### Aside: Validate \\& Hyperparameters\n",
    "**Hyperparameters** are parameters of the model that the user has to provide (as opposed to parameters the model finds such as the weights of a neural network). Examples of this might be the learning rate of the gradient descent method (denoted by `lr` later). Since these parameters have to be provided by the user, we need to decide on its value somehow. In the absence of any external information (eg. someone told us a particular learning rate is really good), we use the validate set (independent of train and test) to set these. In the example of learning rate, we may try the learning rate in increments of `0.1` between `0.1` and `0.9`. Then we choose the hyperparameter which gives us the best performance when assessed against the validate set.\n",
    "\n",
    "*For the purpose of this tutorial, we will not do any hyperparameter tuning, so a validate set was not used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa775ce4-b553-43a8-8572-df13187e2bbe",
   "metadata": {},
   "source": [
    "### 2.5 Model Your Data\n",
    "Let's review what we have done so far:\n",
    "\n",
    "|Pipeline | Our Problem |\n",
    "|---| --- |\n",
    "|1. Identify Your Problem | Classify images of items of clothing |\n",
    "|2. Obtain Your Data | 70,000 labelled images (10 different types) of clothes |\n",
    "|3. Explore Your Data | Class distribution perfectly equal across classes |\n",
    "|4. Preare Your Data | Split 70,000 into 60,000 train and 10,000 test set |\n",
    "\n",
    "Note that we didn't have to do too much cleaning because the data we have is close to *perfect* in many regards. For further details about intricacies of this process, this excellent [textbook](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) provides all the nitty gritty detail. \n",
    "\n",
    "Now comes the exciting part! Building the neural network from scratch.\n",
    "\n",
    "The architecture of the neural network we are going to build was first formulated by Yann LeCun (one of the founding fathers of deep learning) as he was working in Bell Laboratory. Here is the [original paper](https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition) if you are interested. In this instance, we will be building a slightly adapted version that the d2l.ai textbook outlines in [this chapter](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html). (The key difference is that we will be dropping the Gaussian activation function in the final layer).\n",
    "\n",
    "![lenet](./images/lenet.svg)\n",
    "\n",
    "**Figure 1:** The architecture of LeNet. ([source](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html))\n",
    "\n",
    "![lenetsimple](./images/lenet-vert.svg)\n",
    "\n",
    "**Figure 2:** Compact version of the architecture of LeNet. ([source](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html))\n",
    "\n",
    "Okay, so that's a lot. But the building blocks are not complex. There are three main ones. Parenthesis indicates the relevant PyTorch function:\n",
    "1. Convolution Layer (`Conv2d`)\n",
    "2. Pooling Layer (`AvgPool2d`)\n",
    "3. Dense/Linear Layer (`Linear`)\n",
    "\n",
    "#### 2.5.1 Convolution Layers\n",
    "Let's take the first transition (from 28x28 image to the 6@28x28 C1 feature map in the Figure 1) as our example. Figure 2 provides us with 3 bits of vital information:\n",
    "* $5 \\times 5$ means that our convolution window (aka kernel size) is going to be 5\n",
    "* pad 2 means that we will 'pad' our image with two squares of 0s (since it's 28x28, this will make it a 30x30 image)\n",
    "* conv(6) means that we are expecting 6 output layers (that is we pass the image through 6 filters)\n",
    "\n",
    "An additional piece of information:\n",
    "* We are starting with a black and white image, so we are expecting a single input layer\n",
    "\n",
    "Putting this together, the code becomes: `nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size=5, padding=2)`.\n",
    "\n",
    "What does this actually look like though? The animation from the Stanford University [course](https://cs231n.github.io/convolutional-networks/) gives an excellent intuitive picture as to what is happening.\n",
    "\n",
    "Question: What is going to be the output dimension of our image?\n",
    "\n",
    "<font color='red'>Examples! Maybe discuss [cross-correlation](http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html?highlight=cross%20correlation) also</font>.\n",
    "\n",
    "#### 2.5.2 Pooling Layers\n",
    "There are two pieces of information from Figure 2:\n",
    "* $2 \\times 2$ AvgPool means we take $2 \\times 2$ values of the image and we average them\n",
    "* stride 2 means we 'skip' two steps with each window\n",
    "\n",
    "Putting this together, the code becomes: `nn.AvgPool2d(kernel_size=2, stride=2)`.\n",
    "<font color='red'>Examples!</font>.\n",
    "\n",
    "\n",
    "#### 2.5.3 Linear/Dense Layer\n",
    "Let's look at the FC(120) layer in Figure 2. The input is 16 layers of 5x5 images. How many pixels is that? $16 \\times 5 \\times 5 = 400$. We squish each pixel into a fully connected layer with 120 as the output.\n",
    "\n",
    "Putting this together, the code becomes: `nn.Linear(in_features = 16 * 5 * 5, out_features = 120)`.\n",
    "\n",
    "<font color='red'>Examples!</font>.\n",
    "\n",
    "\n",
    "#### 2.5.4 Tying Loose Ends\n",
    "Between each layer, we will use a sigmoid function `nn.Sigmoid()` as our activation function. Note we do not need to put activation functions after pooling layers, only after convolutional and linear layers. \n",
    "\n",
    "To convert from a 2D image representation to a 1D linear representation, we use the `nn.Flatten()` function. Try filling out the `?` below according to the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28461ec8-18dc-4eb4-bc8d-646f94606ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_9956/3250750114.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Brian\\AppData\\Local\\Temp/ipykernel_9956/3250750114.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    ?, ?,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialise LeNet Architecture\n",
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    ?, ?,\n",
    "                    ?, nn.Flatten(),\n",
    "                    nn.Linear(16 * 5 * 5, 120), ?,\n",
    "                    ?, ?, \n",
    "                    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e324b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LeNet Architecture (ans)\n",
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "                    nn.Linear(120, 84), nn.Sigmoid(), \n",
    "                    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf291ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:    \t torch.Size([1, 400])\n",
      "Linear output shape:    \t torch.Size([1, 120])\n",
      "Sigmoid output shape:    \t torch.Size([1, 120])\n",
      "Linear output shape:    \t torch.Size([1, 84])\n",
      "Sigmoid output shape:    \t torch.Size([1, 84])\n",
      "Linear output shape:    \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Show layers\n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:    \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631904b",
   "metadata": {},
   "source": [
    "### 2.5.5 Training with a single mini-batch\n",
    "Before we start with the training process, all the weights in our neural network must be initialised with a particular value. A sensible starting point is using the [Xavier Uniform](https://pytorch.org/docs/stable/nn.init.html#) distribution as outlined in [this paper](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). No further justification will be given in this tutorial, although initial weight values of neural networks can spark fascinating discussions in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c559635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d: # We will only set the weights from linear and Conv2d layers, since pooling layers do not require this\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights) # nb: this takes in a function as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63dcda-8765-4f29-831a-31f2996bd23c",
   "metadata": {},
   "source": [
    "Three key decisions need to be made by the user (hyperparamters in one sense):\n",
    "* Learning Rate - How quickly should our algorithm converge? Too quick and we might *miss* the optimal model; Too slow and it will take a long time to run\n",
    "* Optimiser - what algorithm do we use to find the optimal weights? For this we will use [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
    "* Loss Function - how do we measure the 'correctness' of predictions? For this we will use the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "*No further justification will be provided for these, but one should note this is not the only configuration, nor might this be the best configuration. For example the ADAM optimiser is often preferred over SGD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac42030-22cf-447e-9cb7-8ec96caa4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.9 # Learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr) # Using SGD algorithm ot optimise\n",
    "loss = nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a51d8-d9ca-4951-b31f-65dcae0a2a0b",
   "metadata": {},
   "source": [
    "This next step is just to set the neural network into training mode, and not to actually train it itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acafe86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train() # This doesn't actually train, but sets the network on training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c89f8-b6bd-4a89-a1a7-89ff91e1743c",
   "metadata": {},
   "source": [
    "This next bit is where the training happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fcda976-4ef4-42e3-87c9-8071a64cfa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_iter)) # Pick a single minibatch at random to do the training\n",
    "optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "y_hat = net(X) # Forward pass on the data to make prediction\n",
    "l = loss(y_hat, y) # calculate the loss \n",
    "l.backward() # back propagate the loss to update the weights\n",
    "optimizer.step() # step forward in optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742ecea-ad62-492c-b54d-98a200e4a170",
   "metadata": {},
   "source": [
    "And that's it! Pat yourselves on the back, because you've trained your very first CNN for image recognition.\n",
    "\n",
    "### 2.6 Evaluate Your Model\n",
    "Okay, that's all well and good, but how well does it actually perform? Let's extract some of the accuracies. \n",
    "\n",
    "#### 2.6.1 Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c57655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The mini-batch loss is: \t\t\t\t tensor(640.7407, grad_fn=<MulBackward0>)\n",
      "2. The number of correct training predictions is: \t 23.0\n",
      "3. The number of total training predictions is: \t 256\n",
      "This means we get a training accuracy of  0.08984375\n",
      "The average loss for each example is  2.5028934478759766\n"
     ]
    }
   ],
   "source": [
    "metric_1 = l * X.shape[0]\n",
    "metric_2 = accuracy(y_hat, y)\n",
    "metric_3 = X.shape[0] \n",
    "\n",
    "\n",
    "print(\"1. The mini-batch loss is: \\t\\t\\t\\t\", metric_1)\n",
    "print(\"2. The number of correct training predictions is: \\t\", metric_2)\n",
    "print(\"3. The number of total training predictions is: \\t\", metric_3)\n",
    "\n",
    "print(\"This means we get a training accuracy of \", metric_2/metric_3)\n",
    "print(\"The average loss for each example is \", float(metric_1/metric_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c9366-761d-4cfb-89fb-71a0b3891a4f",
   "metadata": {},
   "source": [
    "#### 2.6.2 Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb7cd830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is:  0.1\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = evaluate_accuracy(net, test_iter)\n",
    "print(\"The testing accuracy is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9e135",
   "metadata": {},
   "source": [
    "The train and test accuracy both hover around 10%. That means the model gets the right label about 1 in 10 times. This is no better than randomly picking labels for each image! However, we have only trained over a single mini batch of data. Of course the performance is going to be low. In reality we need to run it over the entire data at least once. Each time we run over the train data once is called an epoch. In the next section we will talk about scaling this up for more training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2243740",
   "metadata": {},
   "source": [
    "## 3. Scaling Up the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60f006c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_weights) # let's reset the NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70cfbd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.001, train acc 1.000, test acc 0.904\n",
      "2606.5 sec taken \n"
     ]
    }
   ],
   "source": [
    "timer = Timer() \n",
    "num_epochs = 500\n",
    "\n",
    "# Keep track of accuracy for each epoch\n",
    "train_accuracy = np.array([])\n",
    "test_accuracy = np.array([])\n",
    "epochs = np.arange(num_epochs) + 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    metric = Accumulator(3) # define a 3d accumulator\n",
    "    net.train() # set to train\n",
    "    for i, (X, y) in enumerate(train_iter): # Loop thru each mini-batch\n",
    "        timer.start()\n",
    "        optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "        y_hat = net(X) # Forward pass on the data to make prediction\n",
    "        l = loss(y_hat, y) # calculate the loss \n",
    "        l.backward() # back propagate the loss\n",
    "        optimizer.step() # step forward in optimisation\n",
    "        with torch.no_grad():\n",
    "            metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0]) # mini-batch loss,  # matches, # total examples\n",
    "        timer.stop()\n",
    "        train_l = metric[0] / metric[2] # loss per unit \n",
    "        train_acc = metric[1] / metric[2] # training accuracy\n",
    "    test_acc = evaluate_accuracy(net, test_iter)\n",
    "    \n",
    "    train_accuracy = np.append(train_accuracy, train_acc)\n",
    "    test_accuracy = np.append(test_accuracy, test_acc)\n",
    "\n",
    "print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "      f'test acc {test_acc:.3f}')\n",
    "print(f'{timer.sum():.1f} sec taken ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34370039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n",
    "plt.plot(epochs, test_accuracy, label = \"Testing Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77cb53b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945402c0",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS\n",
    "Please run the TWO cells below directly after the preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340c5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "        \n",
    "    def print(self):\n",
    "        print(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: # ensure there's more than one example \n",
    "        y_hat = argmax(y_hat, axis=1) # Choose the column corresponding to theb highest y_hat\n",
    "    cmp = astype(y_hat, y.dtype) == y # see if it matches\n",
    "    return float(reduce_sum(astype(cmp, y.dtype))) #check how many matches there are\n",
    "\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "    if isinstance(net, nn.Module): # Check if net is a nn.Module (type)\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`subsec_normal_distribution_and_squared_loss`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d1d01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef69932",
   "metadata": {},
   "source": [
    "You may notice that if you try to print the `train_iter` or `test_iter`, you'll get some weird object rather than the data itself. The reason for this is that we have opted to load the data via **generator functions** rather than directly. This means that unless the data is required at this moment, we will not need to store it, thus saving memory space. This method is often used in deep learning as the datasets we work with are often huge and cannot be stored properly in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af31f0",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in `PyTorch`\n",
    "\n",
    "In this tutorial, we will be building a Convolutional Neural Network (CNN) from the ground up using the `Python` library `PyTorch`. There are two main competing libraries when it comes to implementing neural networks in `Python`:\n",
    " * `tensorflow`: written with productionisation in mind\n",
    " * `pytorch`: written with experimentation and development of novel methods in mind\n",
    "\n",
    "For basic implementations of neural networks, most people use the package `keras` as a front-end to `tensorflow`. This allows the developer to build simple architectures with easy. `pytorch` does not come with a similar front-end, but its complexity also allows a huge level of flexibility\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "CNNs are neural network architectures which facilitates the processing of image data using a technique called a convolution (or a window function). The very first CNN was pioneered by Yann LeCun -- often known as one of the founding fathers of neural networks -- in 1998 [here](https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition).\n",
    "\n",
    "CNNs have to contend with the particularly challenging structure of image data. For simplicity sake, we will limite ourselves to a 28x28 black and white image. With 28x28 pixels, we have a total of 784 pixels with some sense of local 2D structure. Each pixel has a single channel which represents the lightness (or darkness) of that particular point.\n",
    "\n",
    "**NOTE** This tutorial is heavily dependent on the classes defined in [d2l](https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py), which have been forked for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa8ae1d",
   "metadata": {},
   "source": [
    "The best way to understand the data is by looking at a few examples. Here's one:\n",
    "\n",
    "\n",
    "Which label do you think this image belongs to? \n",
    "\n",
    "The purpose of our model is for the computer to automatically take in each of these images, and spit out the (hopefully correct!) label.\n",
    "\n",
    "Image -> Model -> Label\n",
    "\n",
    "The process of refining the model to produce accurate results is called **training/fitting**, which requires labelled data to do. You are effectively learning the pattern between the input image and the output label.\n",
    "\n",
    "## 3. Train-Validate-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f970e-9e65-428c-bf45-460c94773216",
   "metadata": {},
   "source": [
    "#### ***[Advanced]: Data Types\n",
    "*Ignore this section if you have never used Python before.*\n",
    "\n",
    "##### \\*.1 Typical Data Ingress\n",
    "If you have done any analysis in Python before you may be familiar with a typical data ingression. More often than not you will read in a `.csv` file and then cast it into a `numpy` or `pandas` object. For the R users, this is effectively a dataframe, and for the SAS users, this is simply a dataset item (like `.sas7bdat` objects). Either way, `numpy` and `pandas` objects are relatively straight-forward, you can access any part of it at anytime because the data is stored in your memory (RAM). This is normally accessible by the `print()` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368466c3-165e-4a16-9aa8-7d06a9252518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000239A6D6E100>\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)\n",
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999765fa-e788-423a-975b-fb05ad03b6cf",
   "metadata": {},
   "source": [
    "This is weird. What exactly is this `DataLoader` object?! How do we get the actual data itself?\n",
    "\n",
    "The length of the iterator makes sense.\n",
    "\n",
    "PyTorch comes with two different ways of reading data. The `DataSet` method reads very much like a `numpy` or `pandas` object. You can access individual examples by indexing the object. Let's try that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31d2baef-34f6-441a-ab95-b1b90a6603d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
      "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
      "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
      "          0.0157, 0.0000, 0.0000, 0.0118],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
      "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0471, 0.0392, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
      "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
      "          0.3020, 0.5098, 0.2824, 0.0588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
      "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
      "          0.5529, 0.3451, 0.6745, 0.2588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
      "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
      "          0.4824, 0.7686, 0.8980, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
      "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
      "          0.8745, 0.9608, 0.6784, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
      "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
      "          0.8627, 0.9529, 0.7922, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
      "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
      "          0.8863, 0.7725, 0.8196, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
      "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
      "          0.9608, 0.4667, 0.6549, 0.2196],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
      "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
      "          0.8510, 0.8196, 0.3608, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
      "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
      "          0.8549, 1.0000, 0.3020, 0.0000],\n",
      "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
      "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
      "          0.8784, 0.9569, 0.6235, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
      "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
      "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
      "          0.9137, 0.9333, 0.8431, 0.0000],\n",
      "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
      "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
      "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
      "          0.8627, 0.9098, 0.9647, 0.0000],\n",
      "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
      "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
      "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
      "          0.8706, 0.8941, 0.8824, 0.0000],\n",
      "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
      "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
      "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
      "          0.8745, 0.8784, 0.8980, 0.1137],\n",
      "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
      "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
      "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
      "          0.8627, 0.8667, 0.9020, 0.2627],\n",
      "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
      "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
      "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
      "          0.7098, 0.8039, 0.8078, 0.4510],\n",
      "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
      "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
      "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
      "          0.6549, 0.6941, 0.8235, 0.3608],\n",
      "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
      "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
      "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
      "          0.7529, 0.8471, 0.6667, 0.0000],\n",
      "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
      "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
      "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
      "          0.3882, 0.2275, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
      "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "print(training_data[0]) # Get the first element of the training set.\n",
    "\n",
    "# Free up some memory\n",
    "import gc\n",
    "del training_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1313e-e1c3-432b-88fa-cc3c63bf1f40",
   "metadata": {},
   "source": [
    "The reason for this is simple. The `DataSet` object reads all the necessary data into memory directly. However the `DataLoader` object creates a generator function. That is to say it creates an iterable. The `DataLoader` method will only read the data when necessary. This might seem like a lot of work for these smaller datasets. However, if you try to do this for huge datasets which might take up 10-20gb of memory, the `DataSet` method becomes unwealdy. We can still access individual examples by sampling the iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa06529-0d86-4008-b3cb-0697cf1ecfd9",
   "metadata": {},
   "source": [
    "If we try a similar thing for the `DataLoader` object, we don't get the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bbbe8",
   "metadata": {},
   "source": [
    "## 4. Data Formats\n",
    "`torch.utils.data.DataLoader` format\n",
    "Typically, when you work with datasets, you download it into your directory and you work directly with it. This is fine for smaller datasets, but when you work with massive datasets this can be unworkable. You may not have enough RAM to store the dataset in memory, and often times, you don't *need* to use all the data at the same time. `torch` comes with a useful class called `DataLoader`. Rather than directly download your dataset, it allows you to define an iterable. With an iterable, you only access the data when needed using special functions, ensuring both parallisability and efficiency.\n",
    "\n",
    "First we must determine a batch-size:\n",
    "* Small batch sizes ensures frequent updating of the model (leading to fewer epochs required), but can be more computationally expensive\n",
    "* Large batch sizes are easier to run but can require more epochs to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301d91d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "762c4936",
   "metadata": {},
   "source": [
    "You may get the following error:\n",
    "\n",
    "```\n",
    "C:\\Users\\Brian\\anaconda3\\envs\\ML-sandbox\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
    "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
    "  \n",
    "```\n",
    "\n",
    "This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17939dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000200B84B68B0>\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9ab09",
   "metadata": {},
   "source": [
    "Okay, so it appears that I can't print out the value of `train_iter` since it's a dataloader object. This makes sense since this is a generator object. We need to think of more creative ways of outputting this. First let's think about how many units we expect in `train_iter`. There are 60,000 training examples, and we batch them up into batch sizes of 256 -- this means there should be 235 batches (rounded up as the final batch will have less than 256 units, but that's fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896c0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c85426",
   "metadata": {},
   "source": [
    "This is as expected! In order to print out an individual example or batch, we need to randomly sample from the generator. For example's sake let's get 3 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7af150a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_batches = random.sample(list(train_iter), 3)\n",
    "len(sampled_batches) # shows how many batches there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5b26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea8904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of exampels in the batch of the predictors are:  1024\n",
      "The number of exampels in the batch of the labels are:  1024\n",
      "They should be equal!\n"
     ]
    }
   ],
   "source": [
    "# For argument's sake let's choose the 3rd batch\n",
    "batch_no = 2\n",
    "\n",
    "# The object is a list of lists -- each batch with two entries\n",
    "# 0 denotes the predictors\n",
    "# 1 denotes the labels\n",
    "predictor = sampled_batches[batch_no][0]\n",
    "label = sampled_batches[batch_no][1]\n",
    "\n",
    "print(\"The number of exampels in the batch of the predictors are: \", len(predictor))\n",
    "print(\"The number of exampels in the batch of the labels are: \", len(label))\n",
    "print(\"They should be equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20cc6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the predictor torch.Size([1, 28, 28])\n",
      "The shape of the predictor torch.Size([])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dee2660",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "* Predictor: `[1,28,28]` -- this means it's a tensor with a single channel and 28x28 (if it was RGB colour it would be `[3,28,28]`)\n",
    "* Label: `[]` -- this means it's a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7828c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows an image of:  2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ca34341",
   "metadata": {},
   "source": [
    "### A note on `grad`\n",
    "Each tensor will have an option of grad or nograd. By default it is `false`. When evaluating an NN, this is the option we need, since we don't want it to track the history (required for calculating gradients). However, when training, we need to keep this on so that we can do autodiff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d50ba8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_predictor.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "cnn-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
