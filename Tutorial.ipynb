{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d975d00",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in `PyTorch`\n",
    "\n",
    "This tutorial was heavily borrowed from the [d2l.ia](http://d2l.ai/). Major changes made include:\n",
    "* Simplification and adaptation to cpu-only implementation\n",
    "* Comments and descriptions of the functions\n",
    "\n",
    "Note the sample code in the d2l.ia [repo](https://github.com/d2l-ai/d2l-en) is subject to a modified [MIT License](https://en.wikipedia.org/wiki/MIT_License)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ff1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "# import tute.helper as helper\n",
    "# from tute.helper import Accumulator\n",
    "\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e71401",
   "metadata": {},
   "source": [
    "### <font color='red'>STOP: Run the two helper function cells below before proceeding</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbbfab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# helper = reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb234b3f",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "* MNIST is a repository of images containing hand-written digits (0-9) used as a canonical test for CNNs\n",
    "* Today we will be using a slightly different dataset known as *fashion* MNIST\n",
    "* Fashion MNIST dataset can be obtained by a quick google search and in places such as [Kaggle](https://www.kaggle.com/zalando-research/fashionmnist)\n",
    "\n",
    "#### About Fashion MNIST\n",
    "* Each instance/example is a single image (28x28 pixels -- a total of 784 pixels)\n",
    "* The entire image is represented by a `torch.tensor()` object (effectively a matrix)\n",
    "* The value in each pixel represents the darkness of each pixel (0 to 255)\n",
    "* Each image is categorised as follows:\n",
    "    0. T-shirt/top\n",
    "    1. Trouser\n",
    "    2. Pullover\n",
    "    3. Dress\n",
    "    4. Coat\n",
    "    5. Sandal\n",
    "    6. Shirt\n",
    "    7. Sneaker\n",
    "    8. Bag\n",
    "    9. Ankle Boot\n",
    "* There are 70,000 instances of these images\n",
    "\n",
    "The library `torchvision` provides access to various repositories of training data (such as the Fashion MNIST). Here we write a function to download data from the `torchvision` servers and save it into our local directory. Note this is saved usually in the `anaconda` folder of your personal drive if you are using Anaconda. There is something special about this format we will discuss soon.\n",
    "\n",
    "#### Train-Test Set\n",
    "Best practice for machine learning dictates you separate your labelled data into three groups:\n",
    "* `Train` -- this is the data we use to fit the model (n=60,000)\n",
    "* `Validate` -- we will not be using this today to tune hyper-parameters\n",
    "* `Test` -- this is the data we use to determine the model performance (n=10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ded545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, n_workers=4 , resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=n_workers),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=n_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bbbe8",
   "metadata": {},
   "source": [
    "#### `torch.utils.data.DataLoader` format\n",
    "Typically, when you work with datasets, you download it into your directory and you work directly with it. This is fine for smaller datasets, but when you work with massive datasets this can be unworkable. You may not have enough RAM to store the dataset in memory, and often times, you don't *need* to use all the data at the same time. `torch` comes with a useful class called `DataLoader`. Rather than directly download your dataset, it allows you to define an iterable. With an iterable, you only access the data when needed using special functions, ensuring both parallisability and efficiency.\n",
    "\n",
    "First we must determine a batch-size:\n",
    "* Small batch sizes ensures frequent updating of the model (leading to fewer epochs required), but can be more computationally expensive\n",
    "* Large batch sizes are easier to run but can require more epochs to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301d91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size) # n=60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c4936",
   "metadata": {},
   "source": [
    "You may get the following error:\n",
    "\n",
    "```\n",
    "C:\\Users\\Brian\\anaconda3\\envs\\ML-sandbox\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
    "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
    "  \n",
    "```\n",
    "\n",
    "This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17939dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001829238CAC0>\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9ab09",
   "metadata": {},
   "source": [
    "Okay, so it appears that I can't print out the value of `train_iter` since it's a dataloader object. This makes sense since this is a generator object. We need to think of more creative ways of outputting this. First let's think about how many units we expect in `train_iter`. There are 60,000 training examples, and we batch them up into batch sizes of 256 -- this means there should be 235 batches (rounded up as the final batch will have less than 256 units, but that's fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896c0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c85426",
   "metadata": {},
   "source": [
    "This is as expected! In order to print out an individual example or batch, we need to randomly sample from the generator. For example's sake let's get 3 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7af150a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_batches = random.sample(list(train_iter), 3)\n",
    "len(sampled_batches) # shows how many batches there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5b26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea8904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of exampels in the batch of the predictors are:  256\n",
      "The number of exampels in the batch of the labels are:  256\n",
      "They should be equal!\n"
     ]
    }
   ],
   "source": [
    "# For argument's sake let's choose the 3rd batch\n",
    "batch_no = 2\n",
    "\n",
    "# The object is a list of lists -- each batch with two entries\n",
    "# 0 denotes the predictors\n",
    "# 1 denotes the labels\n",
    "predictor = sampled_batches[batch_no][0]\n",
    "label = sampled_batches[batch_no][1]\n",
    "\n",
    "print(\"The number of exampels in the batch of the predictors are: \", len(predictor))\n",
    "print(\"The number of exampels in the batch of the labels are: \", len(label))\n",
    "print(\"They should be equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20cc6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the predictor torch.Size([1, 28, 28])\n",
      "The shape of the predictor torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Let's choose a single example from the batch\n",
    "single_predictor = predictor[100]\n",
    "single_label = label[100]\n",
    "print(\"The shape of the predictor\", single_predictor.shape)\n",
    "print(\"The shape of the predictor\", single_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee2660",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "* Predictor: `[1,28,28]` -- this means it's a tensor with a single channel and 28x28 (if it was RGB colour it would be `[3,28,28]`)\n",
    "* Label: `[]` -- this means it's a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7828c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows an image of:  1\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(single_predictor[0]) # print only the one channel of BW\n",
    "plt.show()\n",
    "\n",
    "print(\"This shows an image of: \", int(single_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277b6d1",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "There are 3 main tools we will use:\n",
    "1. `Conv2d` -- A convolution thru the image matrix (***cross-correlation operation***)\n",
    "2. `AvgPool2d` -- Pooling layer\n",
    "3. `Linear` -- Linear Neural Network (normal) layer (if you are familar with tensorflow this is known as the dense layer)\n",
    "\n",
    "In between we will be using additional tools such as `Sigmoid` for activation functions and `Flatten` to convert our convolutional matrices into linear inputs.\n",
    "\n",
    "### Convolution\n",
    "There are 4 arguments to the `Conv2d`.\n",
    "1. Input:\n",
    "2. Output:\n",
    "3. Kernel Size:\n",
    "4. Stride:\n",
    "\n",
    "https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html?highlight=cross%20correlation\n",
    "\n",
    "Note that we ignore the final Guassian activation layer from the original paper following the d2l tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e324b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LeNet Architecture\n",
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "                    nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf291ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape:    \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape:    \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:    \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:    \t torch.Size([1, 400])\n",
      "Linear output shape:    \t torch.Size([1, 120])\n",
      "Sigmoid output shape:    \t torch.Size([1, 120])\n",
      "Linear output shape:    \t torch.Size([1, 84])\n",
      "Sigmoid output shape:    \t torch.Size([1, 84])\n",
      "Linear output shape:    \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Show layers\n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:    \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca34341",
   "metadata": {},
   "source": [
    "### A note on `grad`\n",
    "Each tensor will have an option of grad or nograd. By default it is `false`. When evaluating an NN, this is the option we need, since we don't want it to track the history (required for calculating gradients). However, when training, we need to keep this on so that we can do autodiff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d50ba8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_predictor.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95d2aa",
   "metadata": {},
   "source": [
    "We need to set initial NN weights -- usually this is using the Xavier uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631904b",
   "metadata": {},
   "source": [
    "## Training with a single mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c559635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights) # nb: this takes in a function as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c679c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.9 # Learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr) # Using SGD algorithm ot optimise\n",
    "loss = nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "# timer = Timer() \n",
    "num_batches = len(train_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acafe86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = Accumulator(3) # define a 3d accumulator\n",
    "net.train() # This doesn't actually train, but sets the network on training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f972573",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_iter)) # Pick one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "288c31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "y_hat = net(X) # Forward pass on the data to make prediction\n",
    "l = loss(y_hat, y) # calculate the loss \n",
    "l.backward() # back propagate the loss\n",
    "optimizer.step() # step forward in optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c57655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The mini-batch loss is: \t\t\t\t tensor(601.5111, grad_fn=<MulBackward0>)\n",
      "2. The number of correct training predictions is: \t 19.0\n",
      "3. The number of total training predictions is: \t 256\n",
      "This means we get a training accuracy of  0.07421875\n",
      "The average loss for each instance is  2.3496527671813965\n"
     ]
    }
   ],
   "source": [
    "metric_1 = l * X.shape[0]\n",
    "metric_2 = accuracy(y_hat, y)\n",
    "metric_3 = X.shape[0] \n",
    "\n",
    "\n",
    "print(\"1. The mini-batch loss is: \\t\\t\\t\\t\", metric_1)\n",
    "print(\"2. The number of correct training predictions is: \\t\", metric_2)\n",
    "print(\"3. The number of total training predictions is: \\t\", metric_3)\n",
    "\n",
    "print(\"This means we get a training accuracy of \", metric_2/metric_3)\n",
    "print(\"The average loss for each instance is \", float(metric_1/metric_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7cd830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is:  0.1\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = evaluate_accuracy(net, test_iter)\n",
    "print(\"The testing accuracy is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9e135",
   "metadata": {},
   "source": [
    "As you can see both the training and testing accuracy is horrible, floating at around 10%. This is no better than randomly assigning labels to the examples. However, this makes sense, since we only trained on a single batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2243740",
   "metadata": {},
   "source": [
    "## Training with more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f006c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): Sigmoid()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): Sigmoid()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): Sigmoid()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): Sigmoid()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_weights) # let's reset the NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70cfbd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.261, train acc 0.903, test acc 0.883\n",
      "276.6 sec taken \n"
     ]
    }
   ],
   "source": [
    "timer = Timer() \n",
    "num_epochs = 10\n",
    "# animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                             legend=['train loss', 'train acc', 'test acc'])\n",
    "\n",
    "train_accuracy = np.array([])\n",
    "test_accuracy = np.array([])\n",
    "epochs = np.arange(num_epochs) + 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3) # define a 3d accumulator\n",
    "    net.train() # train over one epoch\n",
    "    for i, (X, y) in enumerate(train_iter): # Loop thru each mini-batch\n",
    "        timer.start()\n",
    "        optimizer.zero_grad() # before running the forward/backward pass we need to reset the gradient (otherwise it accumulates)\n",
    "        y_hat = net(X) # Forward pass on the data to make prediction\n",
    "        l = loss(y_hat, y) # calculate the loss \n",
    "        l.backward() # back propagate the loss\n",
    "        optimizer.step() # step forward in optimisation\n",
    "        with torch.no_grad():\n",
    "            metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0]) # mini-batch loss,  # matches, # total examples\n",
    "        timer.stop()\n",
    "        train_l = metric[0] / metric[2] # loss per unit \n",
    "        train_acc = metric[1] / metric[2] # training accuracy\n",
    "        # regularly update animator\n",
    "#         if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "#             animator.add(epoch + (i + 1) / num_batches,\n",
    "#                          (train_l, train_acc, None))\n",
    "    test_acc = evaluate_accuracy(net, test_iter)\n",
    "    \n",
    "    train_accuracy = np.append(train_accuracy, train_acc)\n",
    "    test_accuracy = np.append(test_accuracy, test_acc)\n",
    "#     animator.add(epoch + 1, (None, None, test_acc))\n",
    "print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "      f'test acc {test_acc:.3f}')\n",
    "# print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#       f'on')\n",
    "print(f'{timer.sum():.1f} sec taken ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34370039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA17klEQVR4nO3deXwU9f348dd7j9wnSQhHQFA5FAREBFRUEK1YUbQoiKUerQfWesCvrVdbtfawVuu3thbqgUdV8CpqFS8QxFtBULkEBCQBhBDI5txks/v5/fHZhBBybDCbhcz7+XjMY3dmZ2ffQ3TeM5/PZ94jxhiUUko5lyvWASillIotTQRKKeVwmgiUUsrhNBEopZTDaSJQSimH00SglFIOF7VEICKzRWSniKxs4nMRkQdEZIOIfCkiQ6MVi1JKqaZF84rgcWBcM5+fBfQJT1cBM6MYi1JKqSZELREYY5YAu5tZZQLwpLE+BjJEpGu04lFKKdU4Twx/uzuQX2++ILxse8MVReQq7FUDycnJx/Xv379dAlRKqY5i2bJlu4wxOY19FstEII0sa7TehTHmIeAhgGHDhpmlS5dGMy6llOpwROTbpj6L5aihAqBHvfk8YFuMYlFKKceKZSJ4BbgkPHpoJOAzxuzXLKSUUiq6otY0JCJzgNFAtogUALcDXgBjzCxgPvBDYANQAVwerViUUko1LWqJwBgzpYXPDXBttH5fKaVUZPTOYqWUcjhNBEop5XCaCJRSyuE0ESillMNpIlBKKYfTRKCUUg6niUAppRxOE4FSSjmcJgKllHI4TQRKKeVwmgiUUsrhNBEopZTDaSJQSimH00SglFIOF8tHVSqlVIcVCIYor6rBGEhP9OJyNfZ03r2MMZRV1bCrrJryqhr8gSCVgSAV1UH7vjpIvy6pHNszs81j1USglOrQjDFUB0NUVAWpCASpqKqhvNq+VtWEcLsEj1vwul24XYLX5UIESvwBiisC7C6vpriimj0Vdt4fCFIdDBGonWrs9v2BIGVVNZRX1VBeZdep5XEJnZLjyE6JJysljpyUeJLjPRSVV7GzpIrCMvtaGQg2uy9Xn3K4JgKl1KHNGEPIQDBkCBlDMGQIBEOUVdXYyV9T976iKkggFCIYMtQEDTWhEDXh92VVNZRUBijxByiprMEXfu8PBKkJ2gNz7XcCQdMmsSfHuclIiiMxzo3X7SIunDy8bhdpcV5y0+zBPTnOQ3K8h5R4N0lxHgxQVFZFUVk1u8qq2FVezcbCcsqra8hKjiMnNZ7BeRl0To0nJzwlx3tIinOT6HWT4HWTGOcmKc5NWoK3TfalIU0ESjlEMGQPvG6X4BIQab6pAuyBu6Syhm2+Sr7z+dnu87PdV8mOEj/BELhd4HYJIoJb7HYrqoP4KgN1kz1g11AZCBIMtc1BOcHrIj3RS1qCl7REL9kpcfTOTibR6647u/e6BY/bhdclxHlcdQfXpDgPyeGDdJzHRShkCDRINMGQIS3RQ6fkODKT4shI8hLvcbdJ7AcjTQRKHaSqaoL4KuxBtKomSHVNyE5B+1pVY8+kS/01lPoDlPnD76sC4WU14c/tfEX1vs0OHpfYZhGX4BKBRvJCIBjCHwjts8wlkJMaj8flqjuzD9U700/0uklP9JKe6CUvM4n0bvZ9YpzLJova33TZ5OFxu0iN95CS4CEl3p5NpybYg3ZcuLnG43Lhce+N1+OO8TiXQCUUbYBd62DXevsqLsjsVW/qDaldwdVGsQYqweUBd9tfFWgiUCrKqmqC7CqrZldpFbvKqiiu2NukUerft3mjuDJAcUU1xRWBFtuLG0qOc9cdTFMTvKQmeOiWkVA3nxLvwesWgiEIhs9+g8YQDNrXxnjdLjqnxtMtI5Eu6Ql0TU8gJyU+9gfi9mQMFK6FjYth0xLYsQqKtwC1/2YCGT3t/MoXwdRLnO44SO4Mbo89iLu84Vc3JGfDqTdDj+NbjmHjYnh1Ogy9BEZNb/Nd1ESgVFgoZCivtk0YVYEQlYG9ozUqAkFKKu2Zde2BuyR8ph2oCTcphPZtl95TYQ/+Jf6aJn8zOc5NWriJIzXBQ/eMRAZ0SyMj0UtmchzpiXZ5vMdNvMdFXO3kEtJK1pCQnE5S1mGkpCTjbmFUSuM7HbQHuYoiexab1t0epGKlugJ2fQ0718LujRAK2BhNyE617wHqmrYk/F4gLhmSsiCp076vad1bdyZdss0efGunsh12eafDoftxMORiyO4D2f0g6wjwJtrPgwHw5cOezXunskII1YSn8P4EA/DdV/Do6XDsT+D0OyE5a/84yovgrdvgizn2t7sNbf2/aQQ0EagOLRAMUVwRqBv1UTsCpLC0ih2lfnaWVLGjtIqdJX4KS6uoibANO87tIi3Rnn3HeVx1TRe1zRYJXhdHdUkj+0g7UiQ7NZ7slHi6ym46SQlJOYeTnJGF90DOrIvz4dUbYcOC8AKxTRAZPSHzMEjvASm5kJgJSZmQ2MkeEBM7QXkhbFsOWz+HbZ/D9i8gULF32+44yDjMHnQ6HQ45/eDoCfb7bcUYqNhtm1Z2f2ObVXauhcI1sOdb9jnTdnttk4u4bYISsfP1t4UJf8VAddm+Z+S1svvBT9+IbD9WzYMXfgYmCEnZcPipcPhoO2X0bP67bu/ef7uWVJXCu3+Bj2fC2lfh9Dvg2EtsU5Ix9uD/5m1QVQIn/xJO+eXehNPGxDRxSXiwGjZsmFm6dGmsw1DtzFcZYEeJn/Iq29Zd91ptR5rsrqhmT3k1u8urKSq374vKqylt5mw8I8lLbmoCndPiyU1LoHNqPJlJcSTUjdZw7TNqw3ZMekhL8JLgPYCz5ord8OAIKN9p5xPS7YEl4zD7mjsQjhpvlzcmFIJlj8Hbv7MHitE32+aF4i122vOtfS0paPxgWJ8nAboMgu5D7VlmSmco/hZ2b7Jn4rWvgXJwx8OA82HYT6HH8Hpn4g0YAyVboXQH+PeA37fvVLLNHvyLvgF/8d7vubyQdSR07g85R+197XS4bVJpjVAIqnz237qiyE7F+fasOm84/GQeeOKa/n7BMnj8h9B1MJx9H3Qe0HZt/E3ZuQZe+3/w7Qf2auPkX8InM20zVI8RcM7fofNR3/tnRGSZMWZYo59pIlCxZoyhxF/DjhI7KmXrnkq27K4gf3cFW8KTrzLQ7Da8bjtOu1NyPFnJceH3dsRHZrKXzKQ4OiUIPXYuovM3L+DuORzv6F9H/3/y+uZdA189Bz+8154N1h7Ai7fYg3Cgwh6g+4+HwVPsGWjtgXD3Rnjletj8HvQ+Fc59wDblNCZYA5V7oHK3PSDWf5+Qbg/+Of1bbioxBnauhmWPwxdz7Zlp5wEw7HIYNMk242xbvu9Usavxbbm8kNrFNqN0OsIe+LPCrxk9o9IBuo8vn4P/XgnHToVz/9l4MvMVwMOn2b/Ble/YJNtejIEvn4W3fmOv2uLT4fTb4bjL2+y/UU0EKmaCIcOusiq+8/n5rsRfd7DfEZ6vXd5wRIvXLfTITKJHpyR6hqfc9ARSw0MAkxu8psR7mh4OWbwFlj0By/9j23oTM+3BccD5cN7MqF1u72PDAnhqIpzyKzjtN/t/boxtrvniGdvhWLnHNu8cc6Ft4373HnuwPPOPtk05gqGfbaq63Mb12aOwfYVtqjHhv5m47Bl892Oh6xB7hZOQvu/kTWz/mBt654+w5B444y446fp9P6sqg9njbEL+2VttcgZ+QCqLYeUL9mQgtUubbloTgYo6YwyFpVWs+a6UtdtL+Pq7UtZ8V8qGnaX73dDjcQm5aQnkpsXTNd2ORumSllA3KqVrRiJd0hIOrPOzVrAGNrwNSx+D9W/Zg1CfM23zxpFj4aMHbRNL96Fw0RxIzf2e/wLNqCqFf51gD4bT3gdPfPPr11TZmL+YC+vetB2MfcfB+PshrVv04ozU1s9h9cu2X6LbsdDlGIhLinVULQuF4IXLbewXPQP9fxheHoRnp8K6N+Di56HP6bGNM0o0Eag2EwoZtpf42bCzjA07y/imsKzu/e7y6rr1uqYn0L9LKn27pJKXmWQP9GkJ5KbHk50c33jdlUAlLPojIDD2dwfWXOArgM//Y8/+S7ZCShc75G7oJZDRY991174GL15hO1EvnmsPaJEyBvZsgqKNtjOxuVjn/xo+fQh++ib0HNG6/Skvsmep3Y6N/Rl1R1BdAY+fDYVf287jroPgrd/Chw/AWX+FEVfFOsKo0USgWqWorIoNO8vY5qtkW7GfbcWV4clP/p6KfZpxMpK8HJmTwpGdU+jfJZX+XdPo3yWVjKRmOuQas2M1vPgz2yYNcPgYmPRE0x2n9dWe/S973J5JG2PP+odeCv3Oav4gvf0LeOYi25l5waN2/cYYYw/Im96Dze/btvqSrfazvOFw4WOQnrf/97Z8bJscRlwNZ/2l5X1R0Vf6ne0LADj+Clh4Jxx/JZx9b2zjijJNBKpZwZDhi4JiFn9dyLtf7+TLrT7q/2eRmeSlW0Yi3TISyctM5Ijwgf/IzilkJcdFVKqgScbA0tnw5q0QnwrnzYLS7XZ4ZFYf+PFzTQ/ZCwbswf/9+8Nn/7m2/XzoT5ruSG1MyXaYOwW2rYBjLrDt3zWVEPDvfS3dbseHgx1S2GsU9D7ZdoK+eZvt1D3/Iej7g73bDfjh3yfb159/BPEpB/ZvpNre9i9tgg6UwxGn2Sah1o5QOsRoIlD7MMbwbVEFy77dw5L1hSxZV8ieigAiMKRHBmP6dWZIjwy6ZybSLT2RxLgo3WBUsRteuc6OoT5iLJw/yw5jBHsTz7OX2Pb0i+faYXV7dwBWvwQLf29H0/Q8EU64FvqeeeCjT6or7J2b37wD3gTwJO77mtgJep5gD/45/fdtptm1AZ6/FHastHd9jvmNPagsvAveuxemvghHdsx250PahgWw4hnb9xLJlechThOBw5VV1fBlfjGfb9nD8i3FLM8vrmvPz0qO49S+OZzaL4dT+uSQmdzKJp3mhEKw9n+22cXltQdpt9e+ry6HBbdD2U57I83In+8/TG7nWnjmQntn5sSH4ahzbLPM27+Drcug89H2u31+EPv280AlvH4TfP4EHHaSTQhzLoJjJsH5M2Mbm1JoInCcqpogn39bzAcbdvH+hl18WVBM7Q2zR+QkM7RnJsf2zOTYnhn0y01t/oEZoaA9WJduszcEpXSJrDaKMTD/V/DZw02v0+kI2y7f7dim1ykrtM02BUsh73go+NSWCxhzGwy+KLblEBrzxbO2WStQAck5cO2nbXtXrlIHqLlE0LEbxRwkf3cFr6/czvsbivh0UxH+gH3gxuC8dK4dcyTDenViSF4G6UktNJ2EgrDkr7BhoT3wl27fO1681ri/wMhpzW9n8d02CZzwCxh5jW3PDwbsUMhgtf2d3AEtj+FPyYFL/wcvX2sv5U+/03a8tsfY/wMxeDJ0G2L7PEZco0lAHRL0iuAQ5g8EeXPVdzy3NJ8PNhQB0KdzCicdmc2oI7MZcXgnUlvzIAt/iR25s/4t6DESOvW249ZTu9qz8NQu8N59tk3/lF/Zs/LGmmQ+ngVv3ARDpsKEJu7iPBCh4MF3BaDUIUKvCDqYlVt9PLc0n5eWb6XEX0NeZiLTT+/LxOO6k5d5gDf27N4Ec6ZA0XrbeTbsp42vd+ETtuljyV+hfJetx1L/4PzFszYJ9B9va6S0Zdu9JgGloiKqiUBExgF/B9zAI8aYuxt8ng48BfQMx3KvMeaxaMZ0qNpZ6ueVFdt48fOtrNleQpzHxVkDuzBpWA9OODyrxQdjN+vbD+2dlaEgTP2vvUGqKW4PnPsPW4fl/fttDZsfPWxH96x7E166BnqdDBMf7fDD8ZTqKKL2f6qIuIEHgTOAAuAzEXnFGLO63mrXAquNMeeISA7wtYg8bYypbmSTjuMPBHlr9Q7++3kBS9YVEjIwuEcGv58wgAmDu7fc3l+rpsqWF27s7Hz50/C/G2z54inPQvaRLW9PxI7WScq2VR0ri+HE6+C5S+zduRc9Y4dcKqUOCdE8ZRsObDDGbAQQkbnABKB+IjBAqtg7klKA3UDTdYMdYvOuch5+byOvrNhGaVUN3dITuGb0EZx/bB5Hpgbso/FcmUAzY58L18HXr8Ha+VDwmR22mZxjz+STO9v3wSpbSKz3qfYu3sTM1gV64i9sQbSXr4VN79obwKa+CAlp32v/lVLtK5qJoDuQX2++AGhYaOWfwCvANiAVmGzM/oXUReQq4CqAnj1beDDEIWzlVh8z3/2G17/ajsft4pxB3Zh4XHdG9g43/RSug5nn7S1tkNnL1rDvMgi6DIT4NFj/Jnz9uq37Drau+qjpduRP+S5b4rZsp62BXrkHhl8FZ/7pwG/EGjLFjoz57FHbX9CepXuVUm0imomgsUbrhkOUzgRWAKcBRwBvi8h7xpiSfb5kzEPAQ2BHDbV9qDFiDObbD/i0shv//LCQ99bvIjXew9WnHsHlJ/Wic2q95pVty20ZY3HbUga+fHsn63df2eJptf+0Lq+9+3XENOj3Q0jvHv396HumnZRSh6RoJoICoH65xzzsmX99lwN3GzuGdYOIbAL6A59GMa6DQ3E+vhd+QXrBYg43aRzmvpwTzvwJU0/oRVrDIZ+b3rMjehIz4ZKX7AM96qsut0XbKorgsBO1aUYp1SrRTASfAX1EpDewFbgIuLjBOluAscB7IpIL9AM2RjGm2AuFKH1/Ft7Fv8cTDPFvz0VMTFnJH3x/hy3L4Zi/QUKfvet//To8d6ltBrrkpcbr0cclR3a3r1JKNSJqicAYUyMivwDexA4fnW2MWSUi08KfzwLuAh4Xka+wTUk3GWOaeNbdoS+w42uK5lxNl+LlLAkN4quhd3LpWaeQ4hVbgXPhXTDzRNumP2qGfYDGS9fYdv4fvwDJWbHeBaVUB6R3FreH6nK+nX8fXVc8QIWJ49msazn9ous5onPqvuuV7rDDMb963t7JW7LVjsmfMseWaFZKqQOkdxbHyo5V+D96BPPlsxwWKucd94m4fvhXrho6oPEa/qm5MPERGPJjeONmOPo8OP/fOiZfKRVVmgjaWqASVs3DLH0MKfgUwcsboRFUDrqU8ydMJMEbQZmEI8bAtZ9EP1allEITQdv66gV4bQb4fWz35PFoYCobup3DbRecRN9cbdpRSh2cNBG0lbXzMf+9iu9SB/Krsgl8YQZw0zlHcdvwnt+vDpBSSkWZJoK2sPl9zPOXsdFzBOfuvJ5TBvZmwbkDyE3Ttn2l1MFPE8H3tW0FoWcmk286M7n8l9xxwQguHNaj5e8ppdRBQhPB97FrPTVPnk9hdSKXh27hb5edxil9c2IdlVJKtYomggPlK6DqsXMp8we51nMnD1w+noHdm6kGqpRSBylNBAeifBdlj5yDKdvDLcl/4u9XXECPTgf4ZDCllIoxTQQHYMtTP6dzST5/6PQn/nLFVDKT42IdklJKHTBNBK1UXFJK9rbFvJf8A37z859FdoOYUkodxFyxDuBQ88brL5EkVRx1yo80CSilOgRNBK1QWFpFxao3qBEveceOi3U4SinVJjQRtMLMxd8wihVUdx8B8SmxDkcppdqEJoIIbfdVsuCTZfR1FZB0tF4NKKU6Dk0EEfrHOxs4hRV25sgzYhqLUkq1JR01FIEtRRU891k+r+SsA3pATr9Yh6SUUm1Grwgi8H8L15HgCtK/YhkceTo09lAZpZQ6RGkiaMGGnaW8tHwrtwz04QqU20SglFIdiCaCFty/YD2JXjc/Sl0DLi8cfmqsQ1JKqTaliaAZq7b5eO3L7fx0VG8Sv10EPUfqQ+SVUh2OJoJm3P/2etISPFw5OAF2roI+OlpIKdXxaCJoQjBkWLKukInH5ZFWsNgu1GGjSqkOSBNBE7b7KqkOhujTORU2vA1p3aHzUbEOSyml2pwmgiZs3lUBQO9OXtj4rg4bVUp1WJoImrC5qByAPlWroapEh40qpTosTQRN2LyrnHiPi6zt74HLA4ePjnVISikVFZoImrC5qJxeWcnIhgXQYyQkpMU6JKWUigpNBE3YXFTBoPQK2PEV9NFmIaVUx6WJoBHBkGFLUQWj3V/aBTpsVCnVgWkiaETt0NFjKj+F1G6QOyDWISmlVNRoImjE5l0VJFBFt6KPbLOQDhtVSnVgmggasamonDNdn+EJlMExk2IdjlJKRZUmgkZ8u6ucyd4lmIzD4LCTYh2OUkpFVVQTgYiME5GvRWSDiNzcxDqjRWSFiKwSkXejGU+kSr7byEhZhQz5Mbg0VyqlOraoPapSRNzAg8AZQAHwmYi8YoxZXW+dDOBfwDhjzBYR6RyteFrj6J2v2jdDpsQ2EKWUagfRPN0dDmwwxmw0xlQDc4EJDda5GPivMWYLgDFmZxTjiUgwGGRs1UK+TTsOMnrGOhyllIq6aCaC7kB+vfmC8LL6+gKZIrJYRJaJyCWNbUhErhKRpSKytLCwMErhWkWrFtFDdrK998So/o5SSh0sopkIGhtzaRrMe4DjgLOBM4Hfikjf/b5kzEPGmGHGmGE5OTltH2n931r+FCUmEffR50b1d5RS6mDRYiIQkfEiciAJowDoUW8+D9jWyDpvGGPKjTG7gCXA4AP4rbZRVUrWltd5NXgCPbtkxSwMpZRqT5Ec4C8C1ovIPSLSmiezfAb0EZHeIhIX3s4rDdZ5GThZRDwikgSMANa04jfa1qp5eIJ+XpYx5KYmxCwMpZRqTy2OGjLGTBWRNGAK8JiIGOAxYI4xprSZ79WIyC+ANwE3MNsYs0pEpoU/n2WMWSMibwBfAiHgEWPMyu+/Wwdo+dNs9/bElz4Yl0vvJlZKOUNEw0eNMSUi8iKQCNwInA/8SkQeMMb8o5nvzQfmN1g2q8H8X4G/tjLutrdrA+R/zP/iLuWw7ORYR6OUUu0mkj6Cc0RkHvAO4AWGG2POwrbl/zLK8bWfFU9jxM0T5SPppYlAKeUgkVwRXAjcb4xZUn+hMaZCRH4anbDaWSgIX8zFf9gYtq5Np3eWJgKllHNE0ll8O/Bp7YyIJIpILwBjzMIoxdW+vlkEpdvY1OM8AA7TRKCUcpBIEsHz2I7cWsHwso5jxVOQ2InlCSMB6K1NQ0opB4kkEXjCJSIACL+Pi15I7ayqFNa+BoMmsXFPgASvi86p8bGOSiml2k0kiaBQROpusxWRCcCu6IXUznxbIVgNecezeZd9YL0OHVVKOUkkncXTgKdF5J/YshH5QKM1gQ5Jfp99TcxgU1E5fTunxjYepZRqZ5HcUPYNMFJEUgBp7iayQ1I4EQTjM8jfvZMzjs6NcUBKKdW+IrqhTETOBgYACRJ+fq8x5vdRjKv9hBPBzup4AkGjQ0eVUo4TyQ1ls4DJwHXYpqELgcOiHFf78RcD8G2FF0BvJlNKOU4kncUnGmMuAfYYY+4ETmDfqqKHtnAi+KbE/lP00isCpZTDRJII/OHXChHpBgSA3tELqZ35feBJ5JvdNSR63eSm6dBRpZSzRNJH8L/ws4X/CnyOfbjMw9EMql35fZCYwbdF5RyWlURtH4hSSjlFs4kg/ECahcaYYuBFEXkVSDDG+NojuHbh90FCug4dVUo5VrNNQ8aYEHBfvfmqDpUEACqLMfFp5O+u0I5ipZQjRdJH8JaITJSO2mbi9+H3pNqho9lJsY5GKaXaXSR9BDOAZKBGRPzYIaTGGJMW1cjai99HaVxPQKuOKqWcKZI7izt2w7nfx+6URECrjiqlnKnFRCAipzS2vOGDag5JxoDfR2EgkUSvW6uOKqUcKZKmoV/Ve58ADAeWAadFJaL2VF0GJsi2qjgdOqqUcqxImobOqT8vIj2Ae6IWUXsK1xnKr/DSu4c2CymlnCmSUUMNFQAD2zqQmAgngm/LvdpRrJRyrEj6CP6BvZsYbOIYAnwRxZjaTzgR7A4lcmKWDh1VSjlTJH0ES+u9rwHmGGM+iFI87SucCHwmmZwU7ShWSjlTJIngBcBvjAkCiIhbRJKMMRXRDa0dVBYDUEIymckd5zHMSinVGpH0ESwEEuvNJwILohNOOwtfEZSYJDppIlBKOVQkiSDBGFNWOxN+3zEa1MOJoJQkOiVpIlBKOVMkiaBcRIbWzojIcUBl9EJqR34f1a4kcHlITYjoqZ1KKdXhRHL0uxF4XkS2hee7Yh9deejz+6hwp5CZ5MXl0pvJlFLOFMkNZZ+JSH+gH7bg3FpjTCDqkbUHfzFlkkJmojYLKaWcK5KH118LJBtjVhpjvgJSROTn0Q+tHfh9lJKkI4aUUo4WSR/BleEnlAFgjNkDXBm1iNqTv5jikHYUK6WcLZJE4Kr/UBoRcQMd48jp97E7mKhXBEopR4uks/hN4DkRmYUtNTENeD2qUbUT4/exqyaBTsneWIeilFIxE0kiuAm4CrgG21m8HDty6NAWCoG/hGKTTKY2DSmlHKzFpqHwA+w/BjYCw4CxwJpINi4i40TkaxHZICI3N7Pe8SISFJELIoz7+6sqQTB6V7FSyvGavCIQkb7ARcAUoAh4FsAYMyaSDYf7Eh4EzsCWrv5MRF4xxqxuZL2/YJug2k9teQkdNaSUcrjmrgjWYs/+zzHGjDLG/AMItmLbw4ENxpiNxphqYC4woZH1rgNeBHa2YtvfX12doWQdNaSUcrTmEsFE4DtgkYg8LCJjsX0EkeoO5NebLwgvqyMi3YHzgVnNbUhErhKRpSKytLCwsBUhNKPeFYE2DSmlnKzJRGCMmWeMmQz0BxYD04FcEZkpIj+IYNuNJQ3TYP7/gJtqS1w3E8tDxphhxphhOTk5Efx0BPzFgH0WgSYCpZSTRVJiohx4GnhaRDoBFwI3A2+18NUCoEe9+TxgW4N1hgFzw7cpZAM/FJEaY8xLEUX/fYSvCCrdKSTFuaP+c0opdbBqVclNY8xu4N/hqSWfAX1EpDewFdvxfHGD7fWufS8ijwOvtksSgLpE4E7MpN79ckop5ThRq71sjKkRkV9gRwO5gdnGmFUiMi38ebP9AlHn9xFCiEtOj2kYSikVa1Etwm+MmQ/Mb7Cs0QRgjLksmrHsx++jUpLI1GcVK6UcLpJaQx1TZbF9VrEOHVVKOZxzE4Hfh0/vKlZKKecmAuMvZk8oSa8IlFKO59hEEKwo1jpDSimFgxOB8fsoMVpnSCmlHJsIXH4fPrTOkFJKOTMRBGtw15RTYpLJ1IfSKKUczpmJoKoE0IJzSikFTk0E4YJzJUZHDSmllEMTga0zVOVJJcGrBeeUUs7mzERQWQyASdA6Q0op5cxEEL4icCVmxjgQpZSKPUcnAk9yRmzjUEqpg4CjE0F8sl4RKKWUYxNB0AhJqdpHoJRSjkwEwYo9lJBMlj6LQCmlnJkIAuV78JlkrTOklFI4NBHUVBTbu4r1ZjKllHJmIghVFmvlUaWUCnNkIhC/jxKStc6QUkrh0ETgri7ROkNKKRXmyETgDZTgI5mMJC1BrZRSzksENVV4Q1VUe1Lxup23+0op1ZDzjoR++yyCYFxajANRSqmDgwMTgS0voZVHlVLKcmAiKAbApYlAKaUABycCtxacU0opwJGJwDYNxaV0inEgSil1cHBcIqguKwYgXhOBUkoBDkwElaVFACSna9OQUkqBAxNBddluqo2btBQdPqqUUuDARBAoL7Z1hvRZBEopBTgwEYQqi/VZBEopVY/jEgF+H6X6LAKllKoT1UQgIuNE5GsR2SAiNzfy+Y9F5Mvw9KGIDI5mPADuqhJKSCItUQvOKaUURDERiIgbeBA4CzgamCIiRzdYbRNwqjFmEHAX8FC04qnlCfjwu1NxuyTaP6WUUoeEaF4RDAc2GGM2GmOqgbnAhPorGGM+NMbsCc9+DORFMR4A4mvKqPakRvtnlFLqkBHNRNAdyK83XxBe1pSfAa839oGIXCUiS0VkaWFh4YFHZAyJwTJq4rTOkFJK1YpmImis7cU0uqLIGGwiuKmxz40xDxljhhljhuXk5Bx4RDV+vAQIxes9BEopVcsTxW0XAD3qzecB2xquJCKDgEeAs4wxRVGMp67OkCTqFYFSStWK5hXBZ0AfEektInHARcAr9VcQkZ7Af4GfGGPWRTEWAEyl7Y5wJ2ZE+6eUUuqQEbUrAmNMjYj8AngTcAOzjTGrRGRa+PNZwO+ALOBfIgJQY4wZFq2YKkp2kwzEpWidIaWUqhXNpiGMMfOB+Q2Wzar3/grgimjGUF+5zyYCrTyqlFJ7RTURHGwqwpVHE9OyYhyJUu0rEAhQUFCA3++PdSgqyhISEsjLy8PrjfymWUclgqpS20eQkqGJQDlLQUEBqamp9OrVi3AzrOqAjDEUFRVRUFBA7969I/6eo2oNBcp2A5CWrolAOYvf7ycrK0uTQAcnImRlZbX6ys9RiSBYWYzfeMlM1zuLlfNoEnCGA/k7OyoR4PdRQjIp8Y5qEVNKqWY5KhGI30eZpOiZkVLtqKioiCFDhjBkyBC6dOlC9+7d6+arq6ub/e7SpUu5/vrrW/yNE088sa3CBeCGG26ge/fuhEKhNt3uwcpRp8aeQAl+d0qsw1DKUbKyslixYgUAd9xxBykpKfzyl7+s+7ympgaPp/FD0bBhwxg2rOVbiz788MM2iRUgFAoxb948evTowZIlSxg9enSbbbu+YDCI2+2OyrZby1GJID5QSoknI9ZhKBVTd/5vFau3lbTpNo/ulsbt5wyIeP3LLruMTp06sXz5coYOHcrkyZO58cYbqaysJDExkccee4x+/fqxePFi7r33Xl599VXuuOMOtmzZwsaNG9myZQs33nhj3dVCSkoKZWVlLF68mDvuuIPs7GxWrlzJcccdx1NPPYWIMH/+fGbMmEF2djZDhw5l48aNvPrqq/vFtmjRIgYOHMjkyZOZM2dOXSLYsWMH06ZNY+PGjQDMnDmTE088kSeffJJ7770XEWHQoEH85z//4bLLLmP8+PFccMEF+8V355130rVrV1asWMHq1as577zzyM/Px+/3c8MNN3DVVVcB8MYbb3DrrbcSDAbJzs7m7bffpl+/fnz44Yfk5OQQCoXo27cvH3/8MdnZ2d/nz+esRJAQLGV3Qo+WV1RKRd26detYsGABbrebkpISlixZgsfjYcGCBdx66628+OKL+31n7dq1LFq0iNLSUvr168c111yz33j55cuXs2rVKrp168ZJJ53EBx98wLBhw7j66qtZsmQJvXv3ZsqUKU3GNWfOHKZMmcKECRO49dZbCQQCeL1err/+ek499VTmzZtHMBikrKyMVatW8cc//pEPPviA7Oxsdu/e3eJ+f/rpp6xcubJueOfs2bPp1KkTlZWVHH/88UycOJFQKMSVV15ZF+/u3btxuVxMnTqVp59+mhtvvJEFCxYwePDg750EwGGJIMmUa+VR5XitOXOPpgsvvLCuacTn83HppZeyfv16RIRAINDod84++2zi4+OJj4+nc+fO7Nixg7y8fR9jMnz48LplQ4YMYfPmzaSkpHD44YfXHXynTJnCQw/t/xys6upq5s+fz/33309qaiojRozgrbfe4uyzz+add97hySefBMDtdpOens6TTz7JBRdcUHcw7tSp5aoFw4cP32eM/wMPPMC8efMAyM/PZ/369RQWFnLKKafUrVe73Z/+9KdMmDCBG2+8kdmzZ3P55Ze3+HuRcEwiCAZDpJpySMiIdShKKSA5Obnu/W9/+1vGjBnDvHnz2Lx5c5Pt8vHx8XXv3W43NTU1Ea1jTKMV8Pfzxhtv4PP5OOaYYwCoqKggKSmJs88+u9H1jTGNDj7xeDx1Hc3GmH06xevv9+LFi1mwYAEfffQRSUlJjB49Gr/f3+R2e/ToQW5uLu+88w6ffPIJTz/9dET71RLHjBoqKSnGIyFcSRmxDkUp1YDP56N7d/vcqscff7zNt9+/f382btzI5s2bAXj22WcbXW/OnDk88sgjbN68mc2bN7Np0ybeeustKioqGDt2LDNnzgRsR29JSQljx47lueeeo6jIlq+pbRrq1asXy5YtA+Dll19u8grH5/ORmZlJUlISa9eu5eOPPwbghBNO4N1332XTpk37bBfgiiuuYOrUqUyaNKnNOpsdkwh8e+yTzbzJWnlUqYPNr3/9a2655RZOOukkgsFgm28/MTGRf/3rX4wbN45Ro0aRm5tLevq+zyWpqKjgzTff3OfsPzk5mVGjRvG///2Pv//97yxatIhjjjmG4447jlWrVjFgwABuu+02Tj31VAYPHsyMGTMAuPLKK3n33XcZPnw4n3zyyT5XAfWNGzeOmpoaBg0axG9/+1tGjhwJQE5ODg899BA/+tGPGDx4MJMnT677zrnnnktZWVmbNQsBSKSXTAeLYcOGmaVLl7b6e6uWf8iAl89i9agHOPr0S6MQmVIHrzVr1nDUUUfFOoyYKisrIyUlBWMM1157LX369GH69OmxDqvVli5dyvTp03nvvfeaXKexv7eILGuqzL9jrgjKffbSKjFVS1Ar5UQPP/wwQ4YMYcCAAfh8Pq6++upYh9Rqd999NxMnTuTPf/5zm27XMZ3Fnb22CFNmVucYR6KUioXp06cfklcA9d18883cfPPNbb5dx1wR9MrNgNxjyMjqEutQlFLqoOKYKwKOPN1OSiml9uGYKwKllFKN00SglFIOp4lAKRVV36cMNdi7b+tXF501a1ZdqYe2UFhYiNfr5d///nebbfNQ45w+AqVUTLRUhrolixcvJiUlpe6ZA9OmTWvT+J5//nlGjhzJnDlzojqktLly27F2cEallIqe12+G775q2212OQbOujvi1ZctW8aMGTMoKysjOzubxx9/nK5du/LAAw8wa9YsPB4PRx99NHfffTezZs3C7Xbz1FNP8Y9//IOFCxfWJZPRo0czYsQIFi1aRHFxMY8++ignn3wyFRUVXHbZZaxdu5ajjjqKzZs38+CDDzb6bIM5c+Zw3333cfHFF7N169a6UheNlZdurBR1t27dGD9+PCtXrgTg3nvvpaysjDvuuIPRo0dz4okn8sEHH3DuuefSt29f/vCHP1BdXU1WVhZPP/00ubm5lJWVcd1117F06VJEhNtvv53i4mJWrlzJ/fffD9j7INasWcPf/va37/vX2o8mAqVUuzLGcN111/Hyyy+Tk5PDs88+y2233cbs2bO5++672bRpE/Hx8RQXF5ORkcG0adP2uYpYuHDhPturqanh008/Zf78+dx5550sWLCAf/3rX2RmZvLll1+ycuVKhgwZ0mgs+fn5fPfddwwfPpxJkybx7LPPMmPGjCbLSzdWinrPnj3N7m9xcTHvvvsuAHv27OHjjz9GRHjkkUe45557uO+++7jrrrtIT0/nq6++qlsvLi6OQYMGcc899+D1ennsscei1nyliUApp2nFmXs0VFVVsXLlSs444wzAFnDr2rUrAIMGDeLHP/4x5513Huedd15E2/vRj34EwHHHHVdXVO7999/nhhtuAGDgwIEMGjSo0e/OnTuXSZMmAXDRRRfxs5/9jBkzZvDOO+80Wl66sVLULSWC+nWCCgoKmDx5Mtu3b6e6urquzPSCBQuYO3du3XqZmbYm2mmnncarr77KUUcdRSAQqKuK2tY0ESil2pUxhgEDBvDRRx/t99lrr73GkiVLeOWVV7jrrrtYtWpVi9urLTtdvyx1pDXU5syZw44dO+rKOW/bto3169c3WQa6MfVLTgP4/f59Pq9fcO66665jxowZnHvuuXVPU6uNt7Hfu+KKK/jTn/5E//7927TIXEM6akgp1a7i4+MpLCysSwSBQIBVq1YRCoXIz89nzJgx3HPPPRQXF1NWVkZqaiqlpaWt+o1Ro0bx3HPPAbB69eq6Jpf6vv76a8rLy9m6dWtd2elbbrmFuXPnNlleurFS1Lm5uezcuZOioiKqqqoaffxlrfrltp944om65T/4wQ/45z//WTdfe5UxYsQI8vPzeeaZZ5p9qtr3pYlAKdWuXC4XL7zwAjfddBODBw9myJAhfPjhhwSDQaZOncoxxxzDsccey/Tp08nIyOCcc85h3rx5DBkypNmKm/X9/Oc/p7CwkEGDBvGXv/yFQYMG7Vd2es6cOZx//vn7LJs4cSJz5sxpsrx0Y6WovV4vv/vd7xgxYgTjx4+nf//+TcZ1xx13cOGFF3LyySfv84jJ3/zmN+zZs4eBAwcyePBgFi1aVPfZpEmTOOmkk+qai6LBMWWolXIyp5WhDgaDBAIBEhIS+Oabbxg7dizr1q0jLi4u1qG12vjx45k+fTpjx46N+DutLUOtfQRKqQ6noqKCMWPGEAgEMMYwc+bMQy4JFBcXM3z4cAYPHtyqJHAgNBEopTqc1NRUDvWWg4yMDNatW9cuv6V9BEo5xKHWDKwOzIH8nTURKOUACQkJFBUVaTLo4IwxFBUVkZCQ0KrvadOQUg6Ql5dHQUEBhYWFsQ5FRVlCQgJ5eXmt+o4mAqUcwOv11t3FqlRDUW0aEpFxIvK1iGwQkf0etCnWA+HPvxSRodGMRyml1P6ilghExA08CJwFHA1MEZGjG6x2FtAnPF0FzIxWPEoppRoXzSuC4cAGY8xGY0w1MBeY0GCdCcCTxvoYyBCRrlGMSSmlVAPR7CPoDuTXmy8ARkSwTndge/2VROQq7BUDQJmIfN3Cb2cDu1obcAeg++08Tt133e/WO6ypD6KZCBor3ddw7Fok62CMeQh4KOIfFlna1K3UHZnut/M4dd91v9tWNJuGCoAe9ebzgG0HsI5SSqkoimYi+AzoIyK9RSQOuAh4pcE6rwCXhEcPjQR8xpjtDTeklFIqeqLWNGSMqRGRXwBvAm5gtjFmlYhMC38+C5gP/BDYAFQAbfXkhYibkToY3W/nceq+6363oUOuDLVSSqm2pbWGlFLK4TQRKKWUw3W4RNBSWYuOQkRmi8hOEVlZb1knEXlbRNaHX6P3bLsYEZEeIrJIRNaIyCoRuSG8vEPvu4gkiMinIvJFeL/vDC/v0PtdS0TcIrJcRF4Nz3f4/RaRzSLylYisEJGl4WVR2e8OlQgiLGvRUTwOjGuw7GZgoTGmD7AwPN/R1AD/zxhzFDASuDb8N+7o+14FnGaMGQwMAcaFR9p19P2udQOwpt68U/Z7jDFmSL17B6Ky3x0qERBZWYsOwRizBNjdYPEE4Inw+yeA89ozpvZgjNlujPk8/L4Ue3DoTgff93AZlrLwrDc8GTr4fgOISB5wNvBIvcUdfr+bEJX97miJoKmSFU6RW3sfRvi1c4zjiSoR6QUcC3yCA/Y93DyyAtgJvG2MccR+A/8H/BoI1VvmhP02wFsisixcZgeitN8d7XkEEZWsUIc+EUkBXgRuNMaUiDT2p+9YjDFBYIiIZADzRGRgjEOKOhEZD+w0xiwTkdExDqe9nWSM2SYinYG3RWRttH6oo10ROL1kxY7a6q3h150xjicqRMSLTQJPG2P+G17siH0HMMYUA4uxfUQdfb9PAs4Vkc3Ypt7TROQpOv5+Y4zZFn7dCczDNn1HZb87WiKIpKxFR/YKcGn4/aXAyzGMJSrEnvo/Cqwxxvyt3kcdet9FJCd8JYCIJAKnA2vp4PttjLnFGJNnjOmF/f/5HWPMVDr4fotIsoik1r4HfgCsJEr73eHuLBaRH2LbFGvLWvwxthFFh4jMAUZjy9LuAG4HXgKeA3oCW4ALjTENO5QPaSIyCngP+Iq9bca3YvsJOuy+i8ggbOegG3sC95wx5vcikkUH3u/6wk1DvzTGjO/o+y0ih2OvAsA24T9jjPljtPa7wyUCpZRSrdPRmoaUUkq1kiYCpZRyOE0ESinlcJoIlFLK4TQRKKWUw2kiUI4lIn8WkdEicl5TlWpF5A4R2RquAFk7ZbRhDI+LyAVttT2lDoQmAuVkI7D3H5yKvTehKfeHK0DWTsXtEp1S7UQTgXIcEfmriHwJHA98BFwBzBSR37ViG5eJyMsi8kb4+Re31/tshoisDE831lt+iYh8GX6mwH/qbe4UEflQRDbWXh2ISFcRWRK+AlkpIid/3/1WqikdreicUi0yxvxKRJ4HfgLMABYbY05q5ivTRWRq+P0eY8yY8PvhwECgAvhMRF7DFjm8HHu1IcAnIvIuUA3chi0ktktEOtXbfldgFNAfW0LgBeBi4M3w3aRuIOl777hSTdBEoJzqWGAF9uC7uoV17zfG3NvI8reNMUUAIvJf7MHcAPOMMeX1lp8cXv6CMWYXQIOyAC8ZY0LAahHJDS/7DJgdLrD3kjFmRet3UanIaCJQjiIiQ7BPd8sDdmHPtCVc5/8EY0xlKzbXsD6LofFS6ISXN1XPparBehhjlojIKdgHsvxHRP5qjHmyFbEpFTHtI1COYoxZYYwZAqzDPs70HeDMcCdwa5IAwBnhZ8gmYp8U9QGwBDhPRJLCVSPPx3ZELwQmhYuG0aBpaD8ichi2Dv/D2GqrQ1sZm1IR0ysC5TgikoNt6w+JSH9jTEtNQ/X7CGDv4wHfB/4DHImtDln7gPHHgU/D6zxijFkeXv5H4F0RCQLLgcua+c3RwK9EJACUAZdEtndKtZ5WH1XqAIjIZcAwY8wvYh2LUt+XNg0ppZTD6RWBUko5nF4RKKWUw2kiUEoph9NEoJRSDqeJQCmlHE4TgVJKOdz/B4LWLKL3FzNAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n",
    "plt.plot(epochs, test_accuracy, label = \"Testing Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"# Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77cb53b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945402c0",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS\n",
    "Please run the TWO cells below directly after the preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340c5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "        \n",
    "    def print(self):\n",
    "        print(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: # ensure there's more than one example \n",
    "        y_hat = argmax(y_hat, axis=1) # Choose the column corresponding to theb highest y_hat\n",
    "    cmp = astype(y_hat, y.dtype) == y # see if it matches\n",
    "    return float(reduce_sum(astype(cmp, y.dtype))) #check how many matches there are\n",
    "\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "    if isinstance(net, nn.Module): # Check if net is a nn.Module (type)\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`subsec_normal_distribution_and_squared_loss`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d1d01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef69932",
   "metadata": {},
   "source": [
    "You may notice that if you try to print the `train_iter` or `test_iter`, you'll get some weird object rather than the data itself. The reason for this is that we have opted to load the data via **generator functions** rather than directly. This means that unless the data is required at this moment, we will not need to store it, thus saving memory space. This method is often used in deep learning as the datasets we work with are often huge and cannot be stored properly in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af31f0",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in `PyTorch`\n",
    "\n",
    "In this tutorial, we will be building a Convolutional Neural Network (CNN) from the ground up using the `Python` library `PyTorch`. There are two main competing libraries when it comes to implementing neural networks in `Python`:\n",
    " * `tensorflow`: written with productionisation in mind\n",
    " * `pytorch`: written with experimentation and development of novel methods in mind\n",
    "\n",
    "For basic implementations of neural networks, most people use the package `keras` as a front-end to `tensorflow`. This allows the developer to build simple architectures with easy. `pytorch` does not come with a similar front-end, but its complexity also allows a huge level of flexibility\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "CNNs are neural network architectures which facilitates the processing of image data using a technique called a convolution (or a window function). The very first CNN was pioneered by Yann LeCun -- often known as one of the founding fathers of neural networks -- in 1998 [here](https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition).\n",
    "\n",
    "CNNs have to contend with the particularly challenging structure of image data. For simplicity sake, we will limite ourselves to a 28x28 black and white image. With 28x28 pixels, we have a total of 784 pixels with some sense of local 2D structure. Each pixel has a single channel which represents the lightness (or darkness) of that particular point.\n",
    "\n",
    "**NOTE** This tutorial is heavily dependent on the classes defined in [d2l](https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py), which have been forked for convenience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tute",
   "language": "python",
   "name": "cnn-tute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
